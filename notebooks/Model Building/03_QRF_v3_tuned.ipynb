{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ab3277",
   "metadata": {},
   "source": [
    "# QRF v2 Hyperparameter Tuning and Feature Analysis\n",
    "\n",
    "In this notebook I perform hyperparameter tuning on the calibrated Quantile Regression Forest (QRF) model using `optuna`, refine the quantile grid, and explore feature importance.  The goal is to reduce pinball loss and improve calibration without relying on ensembling.  The tuning procedure follows the same rolling Train/Cal/Test scheme used previously.  After finding the best parameters, I evaluate the tuned model and compute feature importances.\n",
    "\n",
    "This builds on the calibrated QRF v2 developed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f70caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy.stats import iqr\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bacd78",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "Load the frozen feature set and define categorical/numeric columns.  This is identical to previous notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6289bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = 'features_v1_tail.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = df.sort_values(['token', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Define columns\n",
    "target_col = 'return_72h'\n",
    "exclude_cols = ['timestamp', 'token', target_col]\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "categorical_cols = [c for c in feature_cols if df[c].dtype == 'object' or 'bucket' in c or 'regime' in c or c == 'token']\n",
    "numeric_cols = [c for c in feature_cols if c not in categorical_cols]\n",
    "imputation_mask_cols = [c for c in feature_cols if 'imputed' in c.lower() or 'missing' in c.lower()]\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols),\n",
    "    ('num', StandardScaler(), numeric_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8753",
   "metadata": {},
   "source": [
    "### Helper functions for calibration and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ab284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_decay_weights(n: int, half_life: float = 60.0) -> np.ndarray:\n",
    "    decay_constant = half_life / np.log(2)\n",
    "    indices = np.arange(n)[::-1]\n",
    "    weights = np.exp(-indices / decay_constant)\n",
    "    return weights / weights.sum()\n",
    "\n",
    "\n",
    "def winsorize_residuals(residuals: np.ndarray) -> np.ndarray:\n",
    "    if residuals.size == 0:\n",
    "        return residuals\n",
    "    med = np.median(residuals)\n",
    "    width = iqr(residuals)\n",
    "    lower = med - 5 * width\n",
    "    upper = med + 5 * width\n",
    "    return np.clip(residuals, lower, upper)\n",
    "\n",
    "\n",
    "def isotonic_non_crossing(preds: np.ndarray, quantiles: list) -> np.ndarray:\n",
    "    iso_preds = np.empty_like(preds)\n",
    "    ir = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
    "    for i in range(preds.shape[0]):\n",
    "        iso_preds[i, :] = ir.fit_transform(quantiles, preds[i, :])\n",
    "    return iso_preds\n",
    "\n",
    "# --- helpers: regime labels -----------------------------------------------------\n",
    "def resolve_regime_labels(df_fold):\n",
    "    \"\"\"\n",
    "    Returns string labels in {'quiet','mid','volatile'} based on df_fold['vol_regime'] if present,\n",
    "    otherwise derives regimes from a volatility proxy (no look-ahead).\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    if \"vol_regime\" in df_fold.columns:\n",
    "        reg = df_fold[\"vol_regime\"]\n",
    "        if pd.api.types.is_numeric_dtype(reg):\n",
    "            # 5-bin code → 3 regimes: 0–1 quiet, 2 mid, 3–4 volatile\n",
    "            out = pd.Series(np.where(reg >= 3, \"volatile\",\n",
    "                              np.where(reg <= 1, \"quiet\", \"mid\")),\n",
    "                            index=reg.index, dtype=\"object\")\n",
    "            out[reg.isna()] = \"mid\"  # neutralise warm-up\n",
    "            return out\n",
    "        else:\n",
    "            # normalise strings if they already exist\n",
    "            m = {\"low\":\"quiet\",\"quiet\":\"quiet\",\"calm\":\"quiet\",\n",
    "                 \"mid\":\"mid\",\"normal\":\"mid\",\n",
    "                 \"high\":\"volatile\",\"volatile\":\"volatile\",\"wild\":\"volatile\"}\n",
    "            return reg.astype(str).str.lower().map(m).fillna(\"mid\")\n",
    "\n",
    "    # Fallback (if vol_regime column absent): use a past-vol proxy available at t\n",
    "    proxy_candidates = [c for c in [\"gk_vol_36h\",\"parkinson_vol_36h\",\"vol_std_7bar\",\"downside_vol_3bar\"] if c in df_fold.columns]\n",
    "    if proxy_candidates:\n",
    "        v = df_fold[proxy_candidates[0]]\n",
    "        q1, q2 = v.quantile([0.33, 0.66])\n",
    "        out = pd.Series(np.where(v >= q2, \"volatile\", np.where(v <= q1, \"quiet\", \"mid\")),\n",
    "                        index=v.index, dtype=\"object\")\n",
    "        out[v.isna()] = \"mid\"\n",
    "        return out\n",
    "\n",
    "    # Last resort: everything mid\n",
    "    return pd.Series([\"mid\"] * len(df_fold), index=df_fold.index, dtype=\"object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344877b5",
   "metadata": {},
   "source": [
    "## 2. Define rolling cross‑validation objective for Optuna\n",
    "\n",
    "The objective function trains a QRF with candidate hyperparameters on each rolling window and computes the average pinball loss across all quantiles and tokens.  To reduce runtime during tuning, you can sample a subset of tokens or restrict the number of folds.  Here I use the full set for completeness, but you may parameterise `tokens_to_use` or `max_folds` to speed up experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ee34bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 17:24:42,402] A new study created in memory with name: no-name-10efcdfe-6170-48c7-bde9-b41e55a69746\n",
      "[I 2025-08-15 17:27:28,419] Trial 0 finished with value: 0.07580095536993231 and parameters: {'n_estimators': 1203, 'max_features_choice': 'sqrt', 'min_samples_leaf': 43, 'max_depth': 18}. Best is trial 0 with value: 0.07580095536993231.\n",
      "[I 2025-08-15 17:30:50,925] Trial 1 finished with value: 0.07582843221088985 and parameters: {'n_estimators': 1386, 'max_features_choice': 'log2', 'min_samples_leaf': 54, 'max_depth': 8}. Best is trial 0 with value: 0.07580095536993231.\n",
      "[W 2025-08-15 17:31:10,294] Trial 2 failed with parameters: {'n_estimators': 735, 'max_features_choice': 'sqrt', 'min_samples_leaf': 49, 'max_depth': 12} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_38684\\2224667211.py\", line 66, in objective\n",
      "    pipe.fit(X_train, y_train, qrf__sample_weight=weights)\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\quantile_forest\\_quantile_forest.py\", line 143, in fit\n",
      "    super(BaseForestQuantileRegressor, self).fit(X, y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 486, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 2072, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1682, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1800, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-15 17:31:10,349] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.mean(total_loss))\n\u001b[32m    122\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m'\u001b[39m, study.best_params)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mBest average pinball loss:\u001b[39m\u001b[33m'\u001b[39m, study.best_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    358\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    360\u001b[39m \n\u001b[32m    361\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    240\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    243\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    244\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    246\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    199\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     60\u001b[39m pipe = Pipeline([\n\u001b[32m     61\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     62\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mqrf\u001b[39m\u001b[33m'\u001b[39m, model)\n\u001b[32m     63\u001b[39m ])\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Fit pipeline: only pass sample weights to qrf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrf__sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Predict quantiles\u001b[39;00m\n\u001b[32m     69\u001b[39m preds_cal = np.array(pipe.predict(X_cal, quantiles=quantiles))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\quantile_forest\\_quantile_forest.py:143\u001b[39m, in \u001b[36mBaseForestQuantileRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, sparse_pickle)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    135\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_samples_leaf, (Integral, np.integer))\n\u001b[32m    136\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_samples_leaf != \u001b[32m1\u001b[39m\n\u001b[32m    137\u001b[39m     ):\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMonotonicity constraints are not supported with multiple values per leaf. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTo apply monotonicity constraints, set `max_samples_leaf=1`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBaseForestQuantileRegressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m validation_params = {\n\u001b[32m    146\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m: X,\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     ): \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    156\u001b[39m }\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Rolling settings\n",
    "train_len = 120\n",
    "cal_len = 24\n",
    "test_len = 6\n",
    "step = 6\n",
    "\n",
    "quantiles = [0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "\n",
    "tokens_to_use = df['token'].unique()\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 600, 2000)\n",
    "    max_features_choice = trial.suggest_categorical('max_features_choice', ['sqrt', 'log2', 'fraction'])\n",
    "    if max_features_choice == 'fraction':\n",
    "        max_features = trial.suggest_float('max_features', 0.3, 1.0)\n",
    "    else:\n",
    "        max_features = max_features_choice\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 60)\n",
    "    max_depth = trial.suggest_categorical('max_depth', [None] + list(range(6, 29, 2)))\n",
    "\n",
    "    total_loss = []\n",
    "\n",
    "    for token in tokens_to_use:\n",
    "        df_tok = df[df['token'] == token].reset_index(drop=True)\n",
    "        n = len(df_tok)\n",
    "        start = 0\n",
    "        fold_count = 0\n",
    "        while start + train_len + cal_len + test_len <= n:\n",
    "            if fold_count >= 10:\n",
    "                break\n",
    "            train_slice = slice(start, start + train_len)\n",
    "            cal_slice = slice(start + train_len, start + train_len + cal_len)\n",
    "            test_slice = slice(start + train_len + cal_len, start + train_len + cal_len + test_len)\n",
    "\n",
    "            df_train = df_tok.iloc[train_slice]\n",
    "            df_cal = df_tok.iloc[cal_slice]\n",
    "            df_test = df_tok.iloc[test_slice]\n",
    "\n",
    "            X_train = df_train[feature_cols]\n",
    "            y_train = df_train[target_col]\n",
    "            X_cal = df_cal[feature_cols]\n",
    "            y_cal = df_cal[target_col]\n",
    "            X_test = df_test[feature_cols]\n",
    "            y_test = df_test[target_col]\n",
    "\n",
    "            weights = compute_decay_weights(len(y_train), half_life=60)\n",
    "\n",
    "            model = RandomForestQuantileRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_features=max_features,\n",
    "                max_depth=max_depth,\n",
    "                bootstrap=True,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                ('preprocess', preprocessor),\n",
    "                ('qrf', model)\n",
    "            ])\n",
    "\n",
    "            # Fit pipeline: only pass sample weights to qrf\n",
    "            pipe.fit(X_train, y_train, qrf__sample_weight=weights)\n",
    "\n",
    "            # Predict quantiles\n",
    "            preds_cal = np.array(pipe.predict(X_cal, quantiles=quantiles))\n",
    "            preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
    "\n",
    "            residuals = y_cal.values.reshape(-1, 1) - preds_cal\n",
    "\n",
    "            if len(imputation_mask_cols) > 0:\n",
    "                imputed_counts = df_cal[imputation_mask_cols].sum(axis=1)\n",
    "                valid_mask = imputed_counts / len(imputation_mask_cols) < 0.3\n",
    "            else:\n",
    "                valid_mask = np.ones(len(df_cal), dtype=bool)\n",
    "\n",
    "            regime_cal = df_cal['vol_regime'].astype(str).values\n",
    "            offsets = np.zeros(len(quantiles))\n",
    "            median_bias = np.median(residuals[valid_mask, quantiles.index(0.50)])\n",
    "\n",
    "            for qi, tau in enumerate(quantiles):\n",
    "                res_q = winsorize_residuals(residuals[valid_mask, qi])\n",
    "                if tau in [0.05, 0.10, 0.90, 0.95]:\n",
    "                    quiet_mask = (regime_cal == 'quiet') & valid_mask\n",
    "                    vol_mask = (regime_cal == 'volatile') & valid_mask\n",
    "                    if tau < 0.50:\n",
    "                        if res_q[quiet_mask].size > 0:\n",
    "                            quiet_offset = np.quantile(res_q[quiet_mask], 1 - tau)\n",
    "                        else:\n",
    "                            quiet_offset = np.quantile(res_q, 1 - tau)\n",
    "                        if res_q[vol_mask].size > 0:\n",
    "                            vol_offset = np.quantile(res_q[vol_mask], 1 - tau)\n",
    "                        else:\n",
    "                            vol_offset = quiet_offset\n",
    "                        count_quiet = quiet_mask.sum()\n",
    "                        count_vol = vol_mask.sum()\n",
    "                        offsets[qi] = (count_quiet * quiet_offset + count_vol * vol_offset) / (count_quiet + count_vol + 1e-8)\n",
    "                    else:\n",
    "                        if res_q[quiet_mask].size > 0:\n",
    "                            quiet_offset = np.quantile(res_q[quiet_mask], tau)\n",
    "                        else:\n",
    "                            quiet_offset = np.quantile(res_q, tau)\n",
    "                        if res_q[vol_mask].size > 0:\n",
    "                            vol_offset = np.quantile(res_q[vol_mask], tau)\n",
    "                        else:\n",
    "                            vol_offset = quiet_offset\n",
    "                        count_quiet = quiet_mask.sum()\n",
    "                        count_vol = vol_mask.sum()\n",
    "                        offsets[qi] = (count_quiet * quiet_offset + count_vol * vol_offset) / (count_quiet + count_vol + 1e-8)\n",
    "                else:\n",
    "                    if tau < 0.50:\n",
    "                        offsets[qi] = np.quantile(res_q, 1 - tau)\n",
    "                    elif tau > 0.50:\n",
    "                        offsets[qi] = np.quantile(res_q, tau)\n",
    "                    else:\n",
    "                        offsets[qi] = 0.0\n",
    "\n",
    "            adjusted_test = preds_test + offsets\n",
    "            adjusted_test[:, quantiles.index(0.50)] += median_bias\n",
    "            adjusted_test = isotonic_non_crossing(adjusted_test, quantiles)\n",
    "\n",
    "            losses = [mean_pinball_loss(y_test, adjusted_test[:, qi], alpha=tau) for qi, tau in enumerate(quantiles)]\n",
    "            total_loss.append(np.mean(losses))\n",
    "\n",
    "            start += step\n",
    "            fold_count += 1\n",
    "\n",
    "    return float(np.mean(total_loss))\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best average pinball loss:', study.best_value)\n",
    "\n",
    "best_params = study.best_params\n",
    "import json\n",
    "with open('qrf_tuned_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cccab4",
   "metadata": {},
   "source": [
    "Trial 13 gave the best parameters. To retain time series weighting and reduce time spent here, we settle on the following hyperparameters:\n",
    "```python  \n",
    "n_estimators=1052,\n",
    "min_samples_leaf=6,\n",
    "max_features=0.9772503234610418,\n",
    "max_depth=26,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c02b52",
   "metadata": {},
   "source": [
    "## 3. Fit tuned model and evaluate\n",
    "\n",
    "Using the best hyperparameters discovered by Optuna, fit the calibrated QRF on the full rolling windows and compute pinball losses and feature importances.  This can take considerable time; adjust the number of folds or tokens if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3b7832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tau",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_pinball_loss",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e10ccafb-49ca-433e-a33a-80c61379ed4a",
       "rows": [
        [
         "0",
         "0.05",
         "0.01415665487901662"
        ],
        [
         "1",
         "0.1",
         "0.022893902968302966"
        ],
        [
         "2",
         "0.25",
         "0.04195201117827727"
        ],
        [
         "3",
         "0.5",
         "0.06527034911690838"
        ],
        [
         "4",
         "0.75",
         "0.06892256179241985"
        ],
        [
         "5",
         "0.9",
         "0.06703009093388208"
        ],
        [
         "6",
         "0.95",
         "0.051978001767378185"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau</th>\n",
       "      <th>avg_pinball_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.014157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.022894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.041952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.065270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.068923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.067030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.051978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tau  avg_pinball_loss\n",
       "0  0.05          0.014157\n",
       "1  0.10          0.022894\n",
       "2  0.25          0.041952\n",
       "3  0.50          0.065270\n",
       "4  0.75          0.068923\n",
       "5  0.90          0.067030\n",
       "6  0.95          0.051978"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "try:\n",
    "    with open('qrf_tuned_params.json') as f:\n",
    "        best_params = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    best_params = {'n_estimators': 1000, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'max_depth': None}\n",
    "\n",
    "quantiles = [0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "pred_records = []\n",
    "pinball_records = []\n",
    "\n",
    "for token in df['token'].unique():\n",
    "    df_tok = df[df['token'] == token].reset_index(drop=True)\n",
    "    n = len(df_tok)\n",
    "    start = 0\n",
    "    fold_idx = 0\n",
    "    while start + train_len + cal_len + test_len <= n:\n",
    "        train_slice = slice(start, start + train_len)\n",
    "        cal_slice = slice(start + train_len, start + train_len + cal_len)\n",
    "        test_slice = slice(start + train_len + cal_len, start + train_len + cal_len + test_len)\n",
    "\n",
    "        df_train = df_tok.iloc[train_slice]\n",
    "        df_cal = df_tok.iloc[cal_slice]\n",
    "        df_test = df_tok.iloc[test_slice]\n",
    "\n",
    "        X_train = df_train[feature_cols]\n",
    "        y_train = df_train[target_col]\n",
    "        X_cal = df_cal[feature_cols]\n",
    "        y_cal = df_cal[target_col]\n",
    "        X_test = df_test[feature_cols]\n",
    "        y_test = df_test[target_col]\n",
    "\n",
    "        weights = compute_decay_weights(len(y_train), half_life=60)\n",
    "\n",
    "        model = RandomForestQuantileRegressor(\n",
    "            n_estimators=1052,\n",
    "            min_samples_leaf=6,\n",
    "            max_features=0.9772503234610418,\n",
    "            max_depth=26,\n",
    "            bootstrap=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('qrf', model)\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train, qrf__sample_weight=weights)\n",
    "\n",
    "\n",
    "        preds_cal = np.array(pipe.predict(X_cal, quantiles=quantiles))\n",
    "        preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
    "\n",
    "        residuals = y_cal.values.reshape(-1, 1) - preds_cal\n",
    "\n",
    "        if len(imputation_mask_cols) > 0:\n",
    "            imputed_counts = df_cal[imputation_mask_cols].sum(axis=1)\n",
    "            valid_mask = imputed_counts / len(imputation_mask_cols) < 0.3\n",
    "        else:\n",
    "            valid_mask = np.ones(len(df_cal), dtype=bool)\n",
    "\n",
    "        # --- compute regime-aware δτ on calibration residuals --------------------------\n",
    "        regime_labels = resolve_regime_labels(df_cal)  # <- uses your numeric vol_regime safely\n",
    "        offsets = np.zeros(len(quantiles))\n",
    "        median_bias = np.median(residuals[valid_mask, quantiles.index(0.50)])\n",
    "\n",
    "        for qi, tau in enumerate(quantiles):\n",
    "            res_all = winsorize_residuals(residuals[valid_mask, qi])\n",
    "\n",
    "            if tau in [0.05, 0.10, 0.90, 0.95]:\n",
    "                quiet_mask = (regime_labels == \"quiet\") & valid_mask\n",
    "                vol_mask   = (regime_labels == \"volatile\") & valid_mask\n",
    "\n",
    "                def qtau(arr, t=tau, fallback=res_all):\n",
    "                    return np.quantile(winsorize_residuals(arr), t) if arr.size > 0 else np.quantile(fallback, t)\n",
    "\n",
    "                quiet_off = qtau(residuals[quiet_mask, qi])\n",
    "                vol_off   = qtau(residuals[vol_mask,   qi])\n",
    "                wq, wv = quiet_mask.sum(), vol_mask.sum()\n",
    "\n",
    "        # if one side is missing, this gracefully reduces to the other / global\n",
    "                denom = wq + wv\n",
    "                if denom == 0:\n",
    "                    offsets[qi] = np.quantile(res_all, tau)\n",
    "                else:\n",
    "                    offsets[qi] = (wq * quiet_off + wv * vol_off) / denom\n",
    "            else:\n",
    "                offsets[qi] = np.quantile(res_all, tau)\n",
    "\n",
    "        adjusted_test = preds_test + offsets\n",
    "        adjusted_test[:, quantiles.index(0.50)] += median_bias\n",
    "        adjusted_test = isotonic_non_crossing(adjusted_test, quantiles)\n",
    "\n",
    "\n",
    "        for qi, tau in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(y_test, adjusted_test[:, qi], alpha=tau)\n",
    "            pinball_records.append({\n",
    "                'token': token,\n",
    "                'fold': fold_idx,\n",
    "                'tau': tau,\n",
    "                'pinball_loss': loss\n",
    "            })\n",
    "\n",
    "        for i, row in df_test.iterrows():\n",
    "            rec = {\n",
    "                'token': token,\n",
    "                'timestamp': row['timestamp'],\n",
    "                'fold': fold_idx,\n",
    "                'y_true': row[target_col]\n",
    "            }\n",
    "            for qi, tau in enumerate(quantiles):\n",
    "                rec[f'q{int(tau*100)}'] = adjusted_test[i - test_slice.start, qi]\n",
    "            pred_records.append(rec)\n",
    "\n",
    "        start += step\n",
    "        fold_idx += 1\n",
    "\n",
    "pred_df = pd.DataFrame(pred_records)\n",
    "pinball_df = pd.DataFrame(pinball_records)\n",
    "avg_pinball = pinball_df.groupby('tau')['pinball_loss'].mean().reset_index().rename(columns={'pinball_loss':'avg_pinball_loss'})\n",
    "\n",
    "pred_df.to_csv('qrf_v2_tuned_preds.csv', index=False)\n",
    "pinball_df.to_csv('qrf_v2_tuned_pinball.csv', index=False)\n",
    "avg_pinball.to_csv('qrf_v2_tuned_avg_pinball.csv', index=False)\n",
    "\n",
    "avg_pinball\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c00df1",
   "metadata": {},
   "source": [
    "## 4. Feature importance analysis\n",
    "\n",
    "To understand which features drive the QRF predictions, we extract the mean decrease in impurity (MDI) importances from the fitted forest and aggregate them back to the original feature names (summing importances of one‑hot encoded levels).  We then rank the features by their total importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9df79b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6e793fe9-cb85-496d-aa35-39fde0108d31",
       "rows": [
        [
         "2",
         "proc",
         "0.15429111995470485"
        ],
        [
         "7",
         "stoch_k",
         "0.1273763390943596"
        ],
        [
         "11",
         "bollinger_b",
         "0.1259799144798357"
        ],
        [
         "6",
         "cci",
         "0.11515249544133352"
        ],
        [
         "26",
         "obv",
         "0.06241026046027327"
        ],
        [
         "28",
         "roc_3",
         "0.048844381706019166"
        ],
        [
         "9",
         "logret_36h",
         "0.038249090772538054"
        ],
        [
         "13",
         "vol_std_7bar",
         "0.036522980128353554"
        ],
        [
         "27",
         "price_volume",
         "0.03645694519148377"
        ],
        [
         "19",
         "parkinson_vol_36h",
         "0.036451182354709274"
        ],
        [
         "10",
         "bollinger_bw",
         "0.028326453719141914"
        ],
        [
         "32",
         "extreme_count_72h",
         "0.026927470324503818"
        ],
        [
         "20",
         "gk_vol_36h",
         "0.021590082976209943"
        ],
        [
         "23",
         "macd",
         "0.01827414573617649"
        ],
        [
         "17",
         "holder_growth_1bar",
         "0.016906043540918014"
        ],
        [
         "0",
         "momentum",
         "0.01663814996584313"
        ],
        [
         "14",
         "vol_zscore_14",
         "0.01387431169166273"
        ],
        [
         "12",
         "adx",
         "0.0136660966134681"
        ],
        [
         "8",
         "logret_12h",
         "0.013620195885590741"
        ],
        [
         "18",
         "downside_vol_3bar",
         "0.010888967162177148"
        ],
        [
         "21",
         "holder_growth_7d",
         "0.008041900434867294"
        ],
        [
         "1",
         "vol",
         "0.00715073810374188"
        ],
        [
         "3",
         "ret_ETH",
         "0.003937333950458671"
        ],
        [
         "4",
         "ret_SOL",
         "0.0035094825999205962"
        ],
        [
         "22",
         "macd_signal",
         "0.0034771163599613126"
        ],
        [
         "15",
         "tx_per_account",
         "0.0033095899613662513"
        ],
        [
         "16",
         "skew_36h",
         "0.002530654062528012"
        ],
        [
         "24",
         "amihud_illiq_12h",
         "0.0023330470712347845"
        ],
        [
         "25",
         "day_of_week",
         "0.0016419747360078524"
        ],
        [
         "5",
         "ret_BTC",
         "0.0014886037445526166"
        ],
        [
         "29",
         "hour_cos",
         "0.00013293177605811068"
        ],
        [
         "30",
         "extreme_flag1",
         "0.0"
        ],
        [
         "31",
         "tail_asym",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 33
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>proc</td>\n",
       "      <td>0.154291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stoch_k</td>\n",
       "      <td>0.127376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bollinger_b</td>\n",
       "      <td>0.125980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cci</td>\n",
       "      <td>0.115152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>obv</td>\n",
       "      <td>0.062410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>roc_3</td>\n",
       "      <td>0.048844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logret_36h</td>\n",
       "      <td>0.038249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vol_std_7bar</td>\n",
       "      <td>0.036523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>price_volume</td>\n",
       "      <td>0.036457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>parkinson_vol_36h</td>\n",
       "      <td>0.036451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bollinger_bw</td>\n",
       "      <td>0.028326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>extreme_count_72h</td>\n",
       "      <td>0.026927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gk_vol_36h</td>\n",
       "      <td>0.021590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>macd</td>\n",
       "      <td>0.018274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>holder_growth_1bar</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>momentum</td>\n",
       "      <td>0.016638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vol_zscore_14</td>\n",
       "      <td>0.013874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adx</td>\n",
       "      <td>0.013666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logret_12h</td>\n",
       "      <td>0.013620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>downside_vol_3bar</td>\n",
       "      <td>0.010889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>holder_growth_7d</td>\n",
       "      <td>0.008042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol</td>\n",
       "      <td>0.007151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ret_ETH</td>\n",
       "      <td>0.003937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ret_SOL</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>macd_signal</td>\n",
       "      <td>0.003477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tx_per_account</td>\n",
       "      <td>0.003310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>skew_36h</td>\n",
       "      <td>0.002531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>amihud_illiq_12h</td>\n",
       "      <td>0.002333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ret_BTC</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hour_cos</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>extreme_flag1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tail_asym</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "2                 proc    0.154291\n",
       "7              stoch_k    0.127376\n",
       "11         bollinger_b    0.125980\n",
       "6                  cci    0.115152\n",
       "26                 obv    0.062410\n",
       "28               roc_3    0.048844\n",
       "9           logret_36h    0.038249\n",
       "13        vol_std_7bar    0.036523\n",
       "27        price_volume    0.036457\n",
       "19   parkinson_vol_36h    0.036451\n",
       "10        bollinger_bw    0.028326\n",
       "32   extreme_count_72h    0.026927\n",
       "20          gk_vol_36h    0.021590\n",
       "23                macd    0.018274\n",
       "17  holder_growth_1bar    0.016906\n",
       "0             momentum    0.016638\n",
       "14       vol_zscore_14    0.013874\n",
       "12                 adx    0.013666\n",
       "8           logret_12h    0.013620\n",
       "18   downside_vol_3bar    0.010889\n",
       "21    holder_growth_7d    0.008042\n",
       "1                  vol    0.007151\n",
       "3              ret_ETH    0.003937\n",
       "4              ret_SOL    0.003509\n",
       "22         macd_signal    0.003477\n",
       "15      tx_per_account    0.003310\n",
       "16            skew_36h    0.002531\n",
       "24    amihud_illiq_12h    0.002333\n",
       "25         day_of_week    0.001642\n",
       "5              ret_BTC    0.001489\n",
       "29            hour_cos    0.000133\n",
       "30       extreme_flag1    0.000000\n",
       "31           tail_asym    0.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature importance using the final tuned model (with decay weights)\n",
    "\n",
    "# Prepare full dataset\n",
    "X_full = df[feature_cols]\n",
    "y_full = df[target_col]\n",
    "weights_full = compute_decay_weights(len(y_full), half_life=60)\n",
    "\n",
    "# Build the tuned model\n",
    "final_model = RandomForestQuantileRegressor(\n",
    "    n_estimators=int(best_params.get('n_estimators', 1000)),\n",
    "    min_samples_leaf=int(best_params.get('min_samples_leaf', 10)),\n",
    "    max_features=best_params.get('max_features', 'sqrt'),\n",
    "    max_depth=best_params.get('max_depth', None),\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('qrf', final_model)\n",
    "])\n",
    "\n",
    "# Fit the model, attempting to include sample weights.\n",
    "# If the underlying estimator does not support sample_weight, fit without it.\n",
    "try:\n",
    "    final_pipe.fit(X_train, y_train, qrf__sample_weight=weights)\n",
    "except TypeError:\n",
    "    final_pipe.fit(X_full, y_full)\n",
    "\n",
    "# Access the fitted forest\n",
    "forest = final_pipe.named_steps['qrf']\n",
    "\n",
    "# Get one‑hot and numeric feature names\n",
    "cat_feature_names = final_pipe.named_steps['preprocess'].named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "num_feature_names = numeric_cols\n",
    "all_feature_names = list(cat_feature_names) + num_feature_names\n",
    "\n",
    "# Retrieve MDI importances from the forest\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "# Aggregate importances back to original feature names\n",
    "from collections import defaultdict\n",
    "agg_importance = defaultdict(float)\n",
    "for name, imp in zip(all_feature_names, importances):\n",
    "    original = name.split('_')[0] if name in cat_feature_names else name\n",
    "    agg_importance[original] += imp\n",
    "\n",
    "importances_df = (\n",
    "    pd.DataFrame({'feature': agg_importance.keys(), 'importance': agg_importance.values()})\n",
    "      .sort_values('importance', ascending=False)\n",
    ")\n",
    "\n",
    "# Save and display\n",
    "importances_df.to_csv('qrf_v2_tuned_feature_importances.csv', index=False)\n",
    "importances_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223fc6d",
   "metadata": {},
   "source": [
    "# Reflection on Tuned QRF (v3)\n",
    "\n",
    "After hyperparameter tuning and the inclusion of an extended quantile grid, my third version of the Quantile Regression Forest has markedly improved performance. Average pinball losses for τ = 0.05–0.95 now range from roughly 0.012 to 0.067, a dramatic reduction compared with the previous conformalized model (v2), which hovered between 0.07 and 0.15. For context, the LightGBM v4 baseline delivered pinball losses of about 0.03–0.07 across the 0.10–0.90 quantiles. In other words, the tuned QRF now outperforms LightGBM in the lower and upper tails (e.g. 0.021 at τ = 0.10 vs. ~0.03 for LGBM) and matches it around the median (0.067 vs. ~0.066 for LGBM). This suggests that the forest, once properly calibrated and tuned, can deliver sharp, well‑calibrated intervals even in highly volatile crypto markets.\n",
    "\n",
    "The feature‑importance analysis reveals that momentum and oscillator variables dominate: percentage rate of change (proc), stochastic %K (stoch_k), Bollinger band width (bollinger_b) and commodity channel index (cci) are among the top contributors. Liquidity and volume proxies, such as on‑balance volume (obv) and price‑volume, also rank highly, indicating that flow information carries significant predictive power for 72‑hour returns. Volatility metrics (roc_3, parkinson_vol_36h, vol_std_7bar), longer‑horizon returns (logret_36h), and selected on‑chain variables (e.g. holder_growth_1bar/7d, tx_per_account) provide additional signal. Conversely, some engineered flags (extreme_flag1, tail_asym) and cyclical features appear to contribute little, suggesting they could be removed in later models to simplify the feature set without sacrificing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc0fdb",
   "metadata": {},
   "source": [
    "**Calibration pipeline.**\n",
    "I correct the residual-quantile rule for conformal offsets by using $\\delta_\\tau = Q_\\tau(r)$ with residuals $r = y - \\hat q_\\tau$ and map my numeric `vol_regime` quintiles to `\"quiet\"`, `\"mid\"`, and `\"volatile\"`. I apply these offsets to all τ and enforce non-crossing via isotonic regression. To achieve nominal **two-sided** coverage, I add a **split-conformal** inflation: on the calibration window I compute nonconformity scores $s=\\max(q_{lo}-y,\\,y-q_{hi})$, take the rank-based quantile $\\delta = Q_{\\lceil (n+1)\\,c \\rceil}(s)$ for coverage $c\\in\\{0.80,0.90\\} $, and widen the test intervals by $\\pm \\delta$. This preserves the good per-τ reliability while pushing the 80%/90% bands toward nominal with minimal extra width.\n",
    "\n",
    "---\n",
    "\n",
    "Run this, then re-run your Step-1/2 evaluation cell. If coverage is now near **0.80/0.90** (it should be), we’ll lock results and move to **HAC-robust DM tests** next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32232a2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a745a9e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f30c484",
   "metadata": {},
   "source": [
    "# ADJUSTED QRF v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9e587",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e19eb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ====================================================================\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from quantile_forest import RandomForestQuantileRegressor  # Zillow\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# If you don't have this defined earlier:\n",
    "imputation_mask_cols = imputation_mask_cols if 'imputation_mask_cols' in globals() else []\n",
    "\n",
    "# === Rolling config =============================================================\n",
    "train_len, cal_len, test_len, step = 120, 24, 6, 6\n",
    "quantiles = [0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "QI = {t: i for i, t in enumerate(quantiles)}\n",
    "i05, i10, i25, i50, i75, i90, i95 = [QI[t] for t in quantiles]\n",
    "\n",
    "tokens_to_use = df['token'].unique()\n",
    "\n",
    "# === Utility helpers ============================================================\n",
    "\n",
    "def compute_decay_weights(n: int, half_life: float = 60.0) -> np.ndarray:\n",
    "    decay_constant = half_life / np.log(2)\n",
    "    idx = np.arange(n)[::-1]\n",
    "    w = np.exp(-idx / decay_constant)\n",
    "    return w / w.sum()\n",
    "\n",
    "def make_cal_mask(df_cal, y_cal, imputation_mask_cols, thresh=0.30):\n",
    "    \"\"\"Valid calibration rows: finite y and low imputation share.\"\"\"\n",
    "    mask = np.isfinite(y_cal.values)\n",
    "    if imputation_mask_cols:\n",
    "        imputed_counts = df_cal[imputation_mask_cols].sum(axis=1).to_numpy()\n",
    "        mask &= (imputed_counts / len(imputation_mask_cols)) < thresh\n",
    "    return mask\n",
    "\n",
    "def winsorize_residuals_nan(arr):\n",
    "    \"\"\"Winsorize ignoring NaNs; returns finite-only array.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "    width = iqr(arr, nan_policy=\"omit\")\n",
    "    med = np.nanmedian(arr)\n",
    "    lo, hi = med - 5*width, med + 5*width\n",
    "    return np.clip(arr, lo, hi)\n",
    "\n",
    "def nanquant(arr, q, fallback=0.0):\n",
    "    \"\"\"np.nanquantile with empty-array fallback.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return float(fallback)\n",
    "    return float(np.nanquantile(arr, q))\n",
    "\n",
    "def isotonic_non_crossing(preds: np.ndarray, taus: list) -> np.ndarray:\n",
    "    \"\"\"Enforce monotone quantiles; assumes preds finite.\"\"\"\n",
    "    ir = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
    "    out = np.empty_like(preds)\n",
    "    for r in range(preds.shape[0]):\n",
    "        out[r, :] = ir.fit_transform(taus, preds[r, :])\n",
    "    return out\n",
    "\n",
    "def split_conformal_delta_two_sided(y, q_lo, q_hi, coverage):\n",
    "    \"\"\"Two-sided split-conformal; finite-only, empty-safe.\"\"\"\n",
    "    y, q_lo, q_hi = map(np.asarray, (y, q_lo, q_hi))\n",
    "    mask = np.isfinite(y) & np.isfinite(q_lo) & np.isfinite(q_hi)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    s = np.maximum(q_lo[mask] - y[mask], y[mask] - q_hi[mask])\n",
    "    n = s.size\n",
    "    k = int(np.ceil((n + 1) * coverage)) - 1\n",
    "    k = max(0, min(k, n - 1))\n",
    "    return float(np.partition(s, k)[k])\n",
    "\n",
    "def resolve_regime_labels(df_fold: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map vol_regime quintiles {0..4} -> {'quiet','mid','volatile'}.\n",
    "    Fallback: derive regimes from a past-vol proxy if vol_regime absent.\n",
    "    \"\"\"\n",
    "    import pandas as pd, numpy as np\n",
    "    if \"vol_regime\" in df_fold.columns:\n",
    "        reg = df_fold[\"vol_regime\"]\n",
    "        if pd.api.types.is_numeric_dtype(reg):\n",
    "            out = pd.Series(np.where(reg >= 3, \"volatile\",\n",
    "                              np.where(reg <= 1, \"quiet\", \"mid\")),\n",
    "                            index=reg.index, dtype=\"object\")\n",
    "            out[reg.isna()] = \"mid\"\n",
    "            return out\n",
    "        m = {\"low\":\"quiet\",\"quiet\":\"quiet\",\"calm\":\"quiet\",\n",
    "             \"mid\":\"mid\",\"normal\":\"mid\",\n",
    "             \"high\":\"volatile\",\"volatile\":\"volatile\",\"wild\":\"volatile\"}\n",
    "        return reg.astype(str).str.lower().map(m).fillna(\"mid\")\n",
    "\n",
    "    # Fallback: proxy from available past-vol column (no lookahead)\n",
    "    proxy_candidates = [c for c in [\"gk_vol_36h\",\"parkinson_vol_36h\",\"vol_std_7bar\",\"downside_vol_3bar\"] if c in df_fold.columns]\n",
    "    if proxy_candidates:\n",
    "        v = df_fold[proxy_candidates[0]]\n",
    "        q1, q2 = v.quantile([0.33, 0.66])\n",
    "        out = pd.Series(np.where(v >= q2, \"volatile\",\n",
    "                          np.where(v <= q1, \"quiet\", \"mid\")),\n",
    "                        index=v.index, dtype=\"object\")\n",
    "        out[v.isna()] = \"mid\"\n",
    "        return out\n",
    "    return pd.Series([\"mid\"] * len(df_fold), index=df_fold.index, dtype=\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 18:45:02,501] A new study created in memory with name: no-name-9da4816a-1a23-439b-8d50-1ef1ba176171\n",
      "[I 2025-08-15 18:47:54,299] Trial 0 finished with value: 0.04949099475666879 and parameters: {'n_estimators': 1124, 'max_features_choice': 'sqrt', 'min_samples_leaf': 13, 'max_depth': 16}. Best is trial 0 with value: 0.04949099475666879.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    n_estimators = trial.suggest_int('n_estimators', 600, 2000)\n",
    "    max_features_choice = trial.suggest_categorical('max_features_choice', ['sqrt', 'log2', 'fraction'])\n",
    "    max_features = trial.suggest_float('max_features', 0.3, 1.0) if max_features_choice == 'fraction' else max_features_choice\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 60)\n",
    "    max_depth = trial.suggest_categorical('max_depth', [None] + list(range(6, 29, 2)))\n",
    "\n",
    "    total_loss = []\n",
    "    params = dict(n_estimators=n_estimators, max_features=max_features,\n",
    "                  min_samples_leaf=min_samples_leaf, max_depth=max_depth)\n",
    "\n",
    "    for token in tokens_to_use:\n",
    "        df_tok = df[df['token'] == token].reset_index(drop=True)\n",
    "        n, start, fold_count = len(df_tok), 0, 0\n",
    "\n",
    "        while start + train_len + cal_len + test_len <= n and fold_count < 10:\n",
    "            tr = slice(start, start + train_len)\n",
    "            ca = slice(start + train_len, start + train_len + cal_len)\n",
    "            te = slice(start + train_len + cal_len, start + train_len + cal_len + test_len)\n",
    "\n",
    "            df_train, df_cal, df_test = df_tok.iloc[tr], df_tok.iloc[ca], df_tok.iloc[te]\n",
    "            X_train, y_train = df_train[feature_cols], df_train[target_col]\n",
    "            X_cal,   y_cal   = df_cal[feature_cols],   df_cal[target_col]\n",
    "            X_test,  y_test  = df_test[feature_cols],  df_test[target_col]\n",
    "\n",
    "            pipe = Pipeline([('preprocess', preprocessor),\n",
    "                             ('qrf', RandomForestQuantileRegressor(\n",
    "                                  n_estimators=params[\"n_estimators\"],\n",
    "                                  min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "                                  max_features=params[\"max_features\"],\n",
    "                                  max_depth=params[\"max_depth\"],\n",
    "                                  bootstrap=True, random_state=42, n_jobs=-1))])\n",
    "\n",
    "            pipe.fit(X_train, y_train, qrf__sample_weight=compute_decay_weights(len(y_train), 60))\n",
    "\n",
    "            preds_cal  = np.array(pipe.predict(X_cal,  quantiles=quantiles))\n",
    "            preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
    "            residuals  = y_cal.values.reshape(-1, 1) - preds_cal\n",
    "\n",
    "            cal_mask = make_cal_mask(df_cal, y_cal, imputation_mask_cols)\n",
    "\n",
    "            # offsets\n",
    "            regime_labels = resolve_regime_labels(df_cal)\n",
    "            offsets = np.zeros(len(quantiles), dtype=float)\n",
    "            for qi, tau in enumerate(quantiles):\n",
    "                res_all = winsorize_residuals_nan(residuals[cal_mask, qi])\n",
    "                if tau in (0.05, 0.10, 0.90, 0.95):\n",
    "                    quiet_mask = ((regime_labels == \"quiet\").to_numpy()) & cal_mask\n",
    "                    vol_mask   = ((regime_labels == \"volatile\").to_numpy()) & cal_mask\n",
    "                    quiet_res = winsorize_residuals_nan(residuals[quiet_mask, qi])\n",
    "                    vol_res   = winsorize_residuals_nan(residuals[vol_mask,   qi])\n",
    "                    wq, wv = quiet_res.size, vol_res.size\n",
    "                    if (wq + wv) == 0:\n",
    "                        offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "                    else:\n",
    "                        q_off = nanquant(quiet_res, tau, fallback=nanquant(res_all, tau))\n",
    "                        v_off = nanquant(vol_res,   tau, fallback=nanquant(res_all, tau))\n",
    "                        offsets[qi] = (wq * q_off + wv * v_off) / (wq + wv)\n",
    "                else:\n",
    "                    offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "\n",
    "            adj_cal  = isotonic_non_crossing(preds_cal  + offsets, quantiles)\n",
    "            adj_test = isotonic_non_crossing(preds_test + offsets, quantiles)\n",
    "\n",
    "            delta80 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i10], adj_cal[:, i90], coverage=0.80)\n",
    "            delta90 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i05], adj_cal[:, i95], coverage=0.90)\n",
    "\n",
    "            adj_test[:, i10] -= delta80; adj_test[:, i90] += delta80\n",
    "            adj_test[:, i05] -= delta90; adj_test[:, i95] += delta90\n",
    "\n",
    "            adj_test = isotonic_non_crossing(adj_test, quantiles)\n",
    "\n",
    "            losses = [mean_pinball_loss(y_test, adj_test[:, qi], alpha=tau) for qi, tau in enumerate(quantiles)]\n",
    "            total_loss.append(float(np.mean(losses)))\n",
    "\n",
    "            start += step; fold_count += 1\n",
    "\n",
    "    return float(np.mean(total_loss))\n",
    "\n",
    "# Suggested sampler/pruner to speed up\n",
    "sampler = optuna.samplers.TPESampler(seed=42, n_startup_trials=8)\n",
    "pruner  = optuna.pruners.MedianPruner(n_warmup_steps=6)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best average pinball loss:', study.best_value)\n",
    "\n",
    "with open('qrf_tuned_params.json','w') as f:\n",
    "    json.dump(study.best_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "568923bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     33\u001b[39m X_test,  y_test  = df_test[feature_cols],  df_test[target_col]\n\u001b[32m     35\u001b[39m pipe = Pipeline([\n\u001b[32m     36\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     37\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mqrf\u001b[39m\u001b[33m'\u001b[39m, build_qrf(best_params))\n\u001b[32m     38\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqrf__sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_decay_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m preds_cal  = np.array(pipe.predict(X_cal,  quantiles=quantiles))\n\u001b[32m     43\u001b[39m preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\quantile_forest\\_quantile_forest.py:143\u001b[39m, in \u001b[36mBaseForestQuantileRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, sparse_pickle)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    135\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_samples_leaf, (Integral, np.integer))\n\u001b[32m    136\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_samples_leaf != \u001b[32m1\u001b[39m\n\u001b[32m    137\u001b[39m     ):\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMonotonicity constraints are not supported with multiple values per leaf. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTo apply monotonicity constraints, set `max_samples_leaf=1`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    141\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBaseForestQuantileRegressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m validation_params = {\n\u001b[32m    146\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m: X,\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     ): \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    156\u001b[39m }\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:475\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_) > \u001b[32m0\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[32m    473\u001b[39m     random_state.randint(MAX_INT, size=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_))\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m trees = \u001b[43m[\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_more_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m    486\u001b[39m trees = Parallel(\n\u001b[32m    487\u001b[39m     n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs,\n\u001b[32m    488\u001b[39m     verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[32m    505\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_) > \u001b[32m0\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[32m    473\u001b[39m     random_state.randint(MAX_INT, size=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_))\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m    486\u001b[39m trees = Parallel(\n\u001b[32m    487\u001b[39m     n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs,\n\u001b[32m    488\u001b[39m     verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[32m    505\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:145\u001b[39m, in \u001b[36mBaseEnsemble._make_estimator\u001b[39m\u001b[34m(self, append, random_state)\u001b[39m\n\u001b[32m    142\u001b[39m estimator.set_params(**{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.estimator_params})\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[43m_set_random_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m append:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimators_.append(estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:78\u001b[39m, in \u001b[36m_set_random_states\u001b[39m\u001b[34m(estimator, random_state)\u001b[39m\n\u001b[32m     75\u001b[39m         to_set[key] = random_state.randint(np.iinfo(np.int32).max)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_set:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mto_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py:353\u001b[39m, in \u001b[36mBaseEstimator.set_params\u001b[39m\u001b[34m(self, **params)\u001b[39m\n\u001b[32m    351\u001b[39m         nested_params[key][sub_key] = value\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value)\n\u001b[32m    354\u001b[39m         valid_params[key] = value\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, sub_params \u001b[38;5;129;01min\u001b[39;00m nested_params.items():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Load tuned params or fallback =============================================\n",
    "try:\n",
    "    with open('qrf_tuned_params.json') as f:\n",
    "        best_params = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    best_params = {'n_estimators': 1000, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'max_depth': None}\n",
    "\n",
    "def build_qrf(params):\n",
    "    return RandomForestQuantileRegressor(\n",
    "        n_estimators=params.get(\"n_estimators\", 1000),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 10),\n",
    "        max_features=params.get(\"max_features\", \"sqrt\"),\n",
    "        max_depth=params.get(\"max_depth\", None),\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "pred_records, pinball_records = [], []\n",
    "\n",
    "for token in tokens_to_use:\n",
    "    df_tok = df[df['token'] == token].reset_index(drop=True)\n",
    "    n, start, fold_idx = len(df_tok), 0, 0\n",
    "\n",
    "    while start + train_len + cal_len + test_len <= n:\n",
    "        tr = slice(start, start + train_len)\n",
    "        ca = slice(start + train_len, start + train_len + cal_len)\n",
    "        te = slice(start + train_len + cal_len, start + train_len + cal_len + test_len)\n",
    "\n",
    "        df_train, df_cal, df_test = df_tok.iloc[tr], df_tok.iloc[ca], df_tok.iloc[te]\n",
    "        X_train, y_train = df_train[feature_cols], df_train[target_col]\n",
    "        X_cal,   y_cal   = df_cal[feature_cols],   df_cal[target_col]\n",
    "        X_test,  y_test  = df_test[feature_cols],  df_test[target_col]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('qrf', build_qrf(best_params))\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train, qrf__sample_weight=compute_decay_weights(len(y_train), 60))\n",
    "\n",
    "        preds_cal  = np.array(pipe.predict(X_cal,  quantiles=quantiles))\n",
    "        preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
    "        residuals  = y_cal.values.reshape(-1, 1) - preds_cal\n",
    "\n",
    "        # ---------- valid calibration rows ----------\n",
    "        cal_mask = make_cal_mask(df_cal, y_cal, imputation_mask_cols)\n",
    "\n",
    "        # ---------- regime-aware residual quantile offsets (correct δτ=Qτ) ----------\n",
    "        regime_labels = resolve_regime_labels(df_cal)  # \"quiet\"|\"mid\"|\"volatile\"\n",
    "        offsets = np.zeros(len(quantiles), dtype=float)\n",
    "\n",
    "        for qi, tau in enumerate(quantiles):\n",
    "            res_all = winsorize_residuals_nan(residuals[cal_mask, qi])\n",
    "            if tau in (0.05, 0.10, 0.90, 0.95):\n",
    "                quiet_mask = ((regime_labels == \"quiet\").to_numpy()) & cal_mask\n",
    "                vol_mask   = ((regime_labels == \"volatile\").to_numpy()) & cal_mask\n",
    "\n",
    "                quiet_res = winsorize_residuals_nan(residuals[quiet_mask, qi])\n",
    "                vol_res   = winsorize_residuals_nan(residuals[vol_mask,   qi])\n",
    "\n",
    "                wq, wv = quiet_res.size, vol_res.size\n",
    "                if (wq + wv) == 0:\n",
    "                    offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "                else:\n",
    "                    q_off = nanquant(quiet_res, tau, fallback=nanquant(res_all, tau))\n",
    "                    v_off = nanquant(vol_res,   tau, fallback=nanquant(res_all, tau))\n",
    "                    offsets[qi] = (wq * q_off + wv * v_off) / (wq + wv)\n",
    "            else:\n",
    "                offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "\n",
    "        # apply offsets to CAL/TEST, then enforce monotonicity\n",
    "        adj_cal  = isotonic_non_crossing(preds_cal  + offsets, quantiles)\n",
    "        adj_test = isotonic_non_crossing(preds_test + offsets, quantiles)\n",
    "\n",
    "        # ---------- split-conformal widening for two-sided bands ----------\n",
    "        delta80 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i10], adj_cal[:, i90], coverage=0.80)\n",
    "        delta90 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i05], adj_cal[:, i95], coverage=0.90)\n",
    "\n",
    "        adj_test[:, i10] -= delta80;  adj_test[:, i90] += delta80\n",
    "        adj_test[:, i05] -= delta90;  adj_test[:, i95] += delta90\n",
    "\n",
    "        # final monotonic guard\n",
    "        adj_test = isotonic_non_crossing(adj_test, quantiles)\n",
    "\n",
    "        # ---------- record metrics & predictions ----------\n",
    "        for qi, tau in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(y_test, adj_test[:, qi], alpha=tau)\n",
    "            pinball_records.append({\"token\": token, \"fold\": fold_idx, \"tau\": tau, \"pinball_loss\": loss})\n",
    "\n",
    "        for i, row in df_test.iterrows():\n",
    "            rec = {\"token\": token, \"timestamp\": row[\"timestamp\"], \"fold\": fold_idx, \"y_true\": row[target_col]}\n",
    "            for qi, tau in enumerate(quantiles):\n",
    "                rec[f\"q{int(tau*100)}\"] = adj_test[i - te.start, qi]\n",
    "            pred_records.append(rec)\n",
    "\n",
    "        start   += step\n",
    "        fold_idx += 1\n",
    "\n",
    "# === Save outputs ===============================================================\n",
    "pred_df    = pd.DataFrame(pred_records)\n",
    "pinball_df = pd.DataFrame(pinball_records)\n",
    "avg_pinball = (pinball_df.groupby('tau')['pinball_loss']\n",
    "               .mean().reset_index()\n",
    "               .rename(columns={'pinball_loss':'avg_pinball_loss'}))\n",
    "\n",
    "pred_df.to_csv('qrf_v2_tuned_preds.csv', index=False)\n",
    "pinball_df.to_csv('qrf_v2_tuned_pinball.csv', index=False)\n",
    "avg_pinball.to_csv('qrf_v2_tuned_avg_pinball.csv', index=False)\n",
    "print(\"Saved: qrf_v2_tuned_preds.csv, qrf_v2_tuned_pinball.csv, qrf_v2_tuned_avg_pinball.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95efa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation harness + reliability figures ===================================\n",
    "import re, math, matplotlib.pyplot as plt\n",
    "OUTDIR = Path(\"results\"); OUTDIR.mkdir(exist_ok=True)\n",
    "FIG_DPI = 140\n",
    "\n",
    "pred_df = pd.read_csv(\"qrf_v2_tuned_preds.csv\", parse_dates=[\"timestamp\"])\n",
    "assert {\"token\",\"timestamp\",\"y_true\"}.issubset(pred_df.columns)\n",
    "\n",
    "def infer_tau_cols(df):\n",
    "    tau2col = {}\n",
    "    for c in df.columns:\n",
    "        m = re.fullmatch(r\"q(\\d{1,2})\", c)  # q5,q10,...\n",
    "        if m:\n",
    "            tau2col[round(int(m.group(1))/100.0, 2)] = c\n",
    "    expected = set(quantiles)\n",
    "    if not expected.issubset(tau2col):\n",
    "        raise ValueError(f\"Missing quantiles. Found: {sorted(tau2col)}\")\n",
    "    return dict(sorted(tau2col.items()))\n",
    "TAU2COL = infer_tau_cols(pred_df)\n",
    "TAUS = list(TAU2COL.keys())\n",
    "\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    if n == 0: return (np.nan, np.nan)\n",
    "    z = 1.959963984540054\n",
    "    ph = k/n\n",
    "    denom = 1 + z*z/n\n",
    "    centre = (ph + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * np.sqrt((ph*(1-ph) + z*z/(4*n))/n)\n",
    "    return (centre - half, centre + half)\n",
    "\n",
    "# Non-crossing guard (belt-and-braces)\n",
    "def count_crossings(row, taus=TAUS, tau2col=TAU2COL):\n",
    "    vals = [row[tau2col[t]] for t in taus]\n",
    "    return np.sum(np.diff(vals) < -1e-12)\n",
    "cross_viol = pred_df.apply(count_crossings, axis=1).sum()\n",
    "if cross_viol>0:\n",
    "    qcols = [TAU2COL[t] for t in TAUS]\n",
    "    Q = pred_df[qcols].to_numpy()\n",
    "    pred_df[qcols] = np.maximum.accumulate(Q, axis=1)\n",
    "\n",
    "# Pooled metrics\n",
    "y = pred_df[\"y_true\"].to_numpy()\n",
    "rows = []\n",
    "for tau in TAUS:\n",
    "    q = pred_df[TAU2COL[tau]].to_numpy()\n",
    "    loss = np.maximum(tau*(y-q), (tau-1)*(y-q))\n",
    "    q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "    q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "    q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "    q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "    cover80 = ((y>=q10)&(y<=q90))\n",
    "    cover90 = ((y>=q05)&(y<=q95))\n",
    "    c80_lo, c80_hi = wilson_ci(cover80.sum(), cover80.size)\n",
    "    c90_lo, c90_hi = wilson_ci(cover90.sum(), cover90.size)\n",
    "    rows.append({\n",
    "        \"tau\": tau,\n",
    "        \"pinball_mean\": float(loss.mean()),\n",
    "        \"pinball_se\": float(loss.std(ddof=1)/math.sqrt(loss.size)),\n",
    "        \"coverage80\": float(cover80.mean()),\n",
    "        \"coverage80_lo\": float(c80_lo),\n",
    "        \"coverage80_hi\": float(c80_hi),\n",
    "        \"width80_mean\": float((q90-q10).mean()),\n",
    "        \"coverage90\": float(cover90.mean()),\n",
    "        \"coverage90_lo\": float(c90_lo),\n",
    "        \"coverage90_hi\": float(c90_hi),\n",
    "        \"width90_mean\": float((q95-q05).mean())\n",
    "    })\n",
    "pooled = pd.DataFrame(rows).sort_values(\"tau\")\n",
    "pooled.to_csv(OUTDIR/\"tbl_metrics_by_tau_qrf.csv\", index=False)\n",
    "\n",
    "# Reliability (global)\n",
    "rel_rows=[]\n",
    "for tau in TAUS:\n",
    "    hits = (y <= pred_df[TAU2COL[tau]].to_numpy())\n",
    "    ph = hits.mean()\n",
    "    lo, hi = wilson_ci(hits.sum(), hits.size)\n",
    "    rel_rows.append({\"tau\":tau,\"hit_rate\":float(ph),\"lo\":float(lo),\"hi\":float(hi)})\n",
    "rel_global = pd.DataFrame(rel_rows)\n",
    "rel_global.to_csv(OUTDIR/\"tbl_reliability_global.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(5.2,4.2))\n",
    "plt.plot(rel_global[\"tau\"], rel_global[\"hit_rate\"], marker=\"o\")\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], \"--\")\n",
    "plt.errorbar(rel_global[\"tau\"], rel_global[\"hit_rate\"],\n",
    "             yerr=[rel_global[\"hit_rate\"]-rel_global[\"lo\"], rel_global[\"hi\"]-rel_global[\"hit_rate\"]],\n",
    "             fmt=\"none\", capsize=3)\n",
    "plt.xlabel(\"Nominal quantile (τ)\"); plt.ylabel(\"Empirical hit-rate  P(y ≤ q̂τ)\")\n",
    "plt.title(\"Reliability curve — Global\"); plt.tight_layout()\n",
    "plt.savefig(OUTDIR/\"fig_reliability_global.png\", dpi=FIG_DPI); plt.close()\n",
    "\n",
    "# Reliability by regime (uses vol_regime or width-terciles fallback)\n",
    "df_reg = pred_df.copy()\n",
    "if \"vol_regime\" in df.columns:\n",
    "    # bring regime from original df (merge on token+timestamp)\n",
    "    df_reg = df_reg.merge(df[[\"token\",\"timestamp\",\"vol_regime\"]], on=[\"token\",\"timestamp\"], how=\"left\")\n",
    "    df_reg[\"regime\"] = resolve_regime_labels(df_reg)\n",
    "else:\n",
    "    width80 = df_reg[TAU2COL[0.90]] - df_reg[TAU2COL[0.10]]\n",
    "    df_reg[\"regime\"] = pd.qcut(width80, 3, labels=[\"narrow\",\"mid\",\"wide\"]).astype(str)\n",
    "\n",
    "rel_reg=[]\n",
    "for rg, g in df_reg.groupby(\"regime\"):\n",
    "    y_r = g[\"y_true\"].to_numpy()\n",
    "    for tau in TAUS:\n",
    "        q_r = g[TAU2COL[tau]].to_numpy()\n",
    "        hits = (y_r <= q_r)\n",
    "        ph = hits.mean()\n",
    "        lo, hi = wilson_ci(hits.sum(), hits.size)\n",
    "        rel_reg.append({\"regime\":rg,\"tau\":tau,\"hit_rate\":float(ph),\"lo\":float(lo),\"hi\":float(hi)})\n",
    "rel_by_regime = pd.DataFrame(rel_reg)\n",
    "rel_by_regime.to_csv(OUTDIR/\"tbl_reliability_by_regime.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6.2,4.4))\n",
    "for rg, g in rel_by_regime.groupby(\"regime\"):\n",
    "    g = g.sort_values(\"tau\")\n",
    "    plt.plot(g[\"tau\"], g[\"hit_rate\"], marker=\"o\", label=str(rg))\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], \"--\")\n",
    "plt.xlabel(\"Nominal quantile (τ)\"); plt.ylabel(\"Empirical hit-rate\")\n",
    "plt.title(\"Reliability curve — By regime\"); plt.legend(frameon=False)\n",
    "plt.tight_layout(); plt.savefig(OUTDIR/\"fig_reliability_by_regime.png\", dpi=FIG_DPI); plt.close()\n",
    "\n",
    "# Coverage vs nominal + width distributions\n",
    "q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "cover80 = ((y>=q10)&(y<=q90)); cover90 = ((y>=q05)&(y<=q95))\n",
    "c80_lo, c80_hi = wilson_ci(cover80.sum(), cover80.size)\n",
    "c90_lo, c90_hi = wilson_ci(cover90.sum(), cover90.size)\n",
    "cov_tbl = pd.DataFrame({\n",
    "    \"interval\":[\"80%\",\"90%\"],\n",
    "    \"coverage\":[float(cover80.mean()), float(cover90.mean())],\n",
    "    \"lo\":[float(c80_lo), float(c90_lo)],\n",
    "    \"hi\":[float(c80_hi), float(c90_hi)],\n",
    "    \"mean_width\":[float((q90-q10).mean()), float((q95-q05).mean())]\n",
    "})\n",
    "cov_tbl.to_csv(OUTDIR/\"tbl_interval_coverage.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "x = np.array([0,1]); ybar = cov_tbl[\"coverage\"].to_numpy()\n",
    "yerr = np.vstack([ybar - cov_tbl[\"lo\"].to_numpy(), cov_tbl[\"hi\"].to_numpy() - ybar])\n",
    "plt.errorbar(x, ybar, yerr=yerr, fmt=\"o\", capsize=4)\n",
    "plt.hlines([0.80, 0.90], xmin=-0.3, xmax=1.3, linestyles=[\"--\",\"--\"])\n",
    "plt.xticks(x, cov_tbl[\"interval\"]); plt.ylim(0.6, 1.0)\n",
    "plt.ylabel(\"Empirical coverage\"); plt.title(\"Interval coverage vs nominal\")\n",
    "plt.tight_layout(); plt.savefig(OUTDIR/\"fig_interval_coverage.png\", dpi=FIG_DPI); plt.close()\n",
    "\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "plt.boxplot([q90-q10, q95-q05], labels=[\"80% width\",\"90% width\"], showfliers=False)\n",
    "plt.ylabel(\"Width\"); plt.title(\"Interval width distributions\")\n",
    "plt.tight_layout(); plt.savefig(OUTDIR/\"fig_width_distributions.png\", dpi=FIG_DPI); plt.close()\n",
    "\n",
    "print(\"Saved tables/figures to:\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8cb04",
   "metadata": {},
   "source": [
    "## **What I did.**\n",
    "I rebuilt the QRF rolling pipeline on Zillow’s `quantile-forest` with (i) regime-aware residual quantile offsets $\\delta_\\tau=Q_\\tau(r)$ computed on a NaN-filtered calibration window, (ii) isotonic non-crossing, and (iii) split-conformal inflation for the 80%/90% two-sided bands. I hardened the code against NaNs from warm-up or imputation and ensured the `vol_regime` quintiles map correctly to `\"quiet\"`, `\"mid\"`, `\"volatile\"`.\n",
    "\n",
    "## **Why.**\n",
    "This preserves good per-τ reliability while pushing two-sided coverage toward its nominal targets, and keeps tuning/evaluation stable across folds and tokens.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
