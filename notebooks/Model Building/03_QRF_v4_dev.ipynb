{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727a9259",
   "metadata": {},
   "source": [
    "# QRF V4: lock evaluation + bug guards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea605cc",
   "metadata": {},
   "source": [
    "I implemented a single evaluation harness that reads my QRF v3 predictions and computes pooled (all tokens) and per-token metrics across œÑ ‚àà {0.05,‚Ä¶,0.95}. It hard-checks two invariants: (i) pinball loss is non-negative, and (ii) quantiles do not cross (q05 ‚â§ q10 ‚â§ ‚Ä¶ ‚â§ q95). If any crossings slip through, I apply a monotonicity fix (cumulative max) purely for reporting. For intervals, I report empirical coverage (80%: q10‚Äìq90; 90%: q05‚Äìq95) with Wilson binomial CIs, and mean interval widths. I export two Quarto-ready tables: tbl_metrics_by_tau_qrf.csv (pooled by œÑ) and tbl_metrics_by_token_qrf.csv (per token √ó œÑ).\n",
    "\n",
    "Why.\n",
    "This locks a trustworthy baseline for all subsequent comparisons and figures (calibration, significance, sharpness). The non-crossing + non-negativity checks prevent silent bugs from contaminating calibration and DM tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16345c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-crossing violations: 0\n",
      "Saved pooled metrics ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_metrics_by_tau_qrf.csv\n",
      "Saved per-token metrics ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_metrics_by_token_qrf.csv\n",
      "   tau  pinball_mean  pinball_se  coverage80  coverage80_lo  coverage80_hi  width80_mean  coverage90  coverage90_lo  coverage90_hi  width90_mean\n",
      "0.0500        0.0141      0.0008      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.1000        0.0224      0.0014      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.2500        0.0416      0.0033      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.5000        0.0610      0.0063      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.7500        0.0716      0.0091      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.9000        0.0660      0.0097      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n",
      "0.9500        0.0478      0.0075      0.7664         0.7516         0.7806        0.4285      0.8781         0.8665         0.8889        0.5993\n"
     ]
    }
   ],
   "source": [
    "# === Step 1 ¬∑ Evaluation harness + sanity guards =================================\n",
    "import re, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- CONFIG --------------------------------------------------------------------\n",
    "PRED_PATH = Path(\"qrf_v2_tuned_preds.csv\")   # update if needed\n",
    "MODEL_NAME = \"QRF_v3\"\n",
    "OUTDIR = Path(\"results\"); OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- 1) Load predictions -------------------------------------------------------\n",
    "pred_df = pd.read_csv(PRED_PATH, parse_dates=[\"timestamp\"])\n",
    "assert {\"token\",\"timestamp\",\"y_true\"}.issubset(pred_df.columns), \"pred_df must have token, timestamp, y_true\"\n",
    "\n",
    "# ---- 2) Infer quantile columns (expects q5,q10,q25,q50,q75,q90,q95) ------------\n",
    "def infer_tau_cols(df):\n",
    "    tau2col = {}\n",
    "    for c in df.columns:\n",
    "        m = re.fullmatch(r\"q(\\d{1,2})\", c)  # q5, q10, q25, ...\n",
    "        if m:\n",
    "            tau = int(m.group(1)) / 100.0\n",
    "            tau2col[round(tau, 2)] = c\n",
    "    if not {0.05,0.10,0.25,0.50,0.75,0.90,0.95}.issubset(tau2col):\n",
    "        raise ValueError(f\"Missing expected quantile columns. Found: {sorted(tau2col.items())}\")\n",
    "    return dict(sorted(tau2col.items()))\n",
    "TAU2COL = infer_tau_cols(pred_df)\n",
    "TAUS = list(TAU2COL.keys())\n",
    "\n",
    "# ---- 3) Sanity: quantile non-crossing check -----------------------------------\n",
    "def count_crossings(row, taus=TAUS, tau2col=TAU2COL):\n",
    "    vals = [row[tau2col[t]] for t in taus]\n",
    "    return np.sum(np.diff(vals) < -1e-12)\n",
    "\n",
    "cross_viol = pred_df.apply(count_crossings, axis=1).sum()\n",
    "print(f\"Non-crossing violations: {cross_viol:,}\")\n",
    "\n",
    "# Optional quick-fix (monotone enforce): cumulative max over taus\n",
    "# (You already do isotonic during inference; this is a belt-and-braces guard.)\n",
    "if cross_viol > 0:\n",
    "    qcols = [TAU2COL[t] for t in TAUS]\n",
    "    Q = pred_df[qcols].to_numpy()\n",
    "    Q_fix = np.maximum.accumulate(Q, axis=1)\n",
    "    pred_df[qcols] = Q_fix\n",
    "    cross_viol_after = pred_df.apply(count_crossings, axis=1).sum()\n",
    "    print(f\"After monotone fix, violations: {cross_viol_after:,}\")\n",
    "\n",
    "# ---- 4) Pinball loss utilities -------------------------------------------------\n",
    "def pinball_loss_vec(y, q, tau):\n",
    "    # proper quantile loss, vectorised\n",
    "    diff = y - q\n",
    "    return np.maximum(tau*diff, (tau-1)*diff)\n",
    "\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    if n == 0: \n",
    "        return (np.nan, np.nan)\n",
    "    from math import sqrt\n",
    "    z = 1.959963984540054 if alpha==0.05 else 1.2815515655446004  # 95% default\n",
    "    phat = k/n\n",
    "    denom = 1 + z**2/n\n",
    "    centre = (phat + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * sqrt((phat*(1-phat) + z*z/(4*n)) / n)\n",
    "    return (centre - half, centre + half)\n",
    "\n",
    "# ---- 5) Compute pooled metrics by œÑ --------------------------------------------\n",
    "rows = []\n",
    "y = pred_df[\"y_true\"].to_numpy()\n",
    "for tau in TAUS:\n",
    "    col = TAU2COL[tau]\n",
    "    q = pred_df[col].to_numpy()\n",
    "\n",
    "    loss = pinball_loss_vec(y, q, tau)\n",
    "    # assert non-negativity up to tiny fp tolerance\n",
    "    assert (loss >= -1e-12).all(), f\"Negative pinball detected at tau={tau}; check your pipeline.\"\n",
    "\n",
    "    # widths & coverage for 80% and 90% intervals\n",
    "    q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "    q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "    q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "    q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "    cover80_mask = (y >= q10) & (y <= q90)\n",
    "    cover90_mask = (y >= q05) & (y <= q95)\n",
    "\n",
    "    n = len(y)\n",
    "    c80 = cover80_mask.mean()\n",
    "    c90 = cover90_mask.mean()\n",
    "    c80_lo, c80_hi = wilson_ci(cover80_mask.sum(), n)\n",
    "    c90_lo, c90_hi = wilson_ci(cover90_mask.sum(), n)\n",
    "\n",
    "    width80 = (q90 - q10).mean()\n",
    "    width90 = (q95 - q05).mean()\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tau\": tau,\n",
    "        \"pinball_mean\": float(loss.mean()),\n",
    "        \"pinball_se\": float(loss.std(ddof=1) / math.sqrt(n)),\n",
    "        \"coverage80\": float(c80),\n",
    "        \"coverage80_lo\": float(c80_lo),\n",
    "        \"coverage80_hi\": float(c80_hi),\n",
    "        \"width80_mean\": float(width80),\n",
    "        \"coverage90\": float(c90),\n",
    "        \"coverage90_lo\": float(c90_lo),\n",
    "        \"coverage90_hi\": float(c90_hi),\n",
    "        \"width90_mean\": float(width90),\n",
    "        \"n_obs\": int(n)\n",
    "    })\n",
    "\n",
    "pooled_metrics = pd.DataFrame(rows).sort_values([\"tau\"])\n",
    "pooled_path = OUTDIR / \"tbl_metrics_by_tau_qrf.csv\"\n",
    "pooled_metrics.to_csv(pooled_path, index=False)\n",
    "print(f\"Saved pooled metrics ‚Üí {pooled_path.resolve()}\")\n",
    "\n",
    "# ---- 6) Per-token metrics (for appendix & DM later) ----------------------------\n",
    "bytok = []\n",
    "for (tok), g in pred_df.groupby(\"token\", sort=False):\n",
    "    y_t = g[\"y_true\"].to_numpy()\n",
    "    for tau in TAUS:\n",
    "        q_t = g[TAU2COL[tau]].to_numpy()\n",
    "        loss = pinball_loss_vec(y_t, q_t, tau)\n",
    "        assert (loss >= -1e-12).all(), f\"Negative pinball for token={tok}, tau={tau}\"\n",
    "\n",
    "        q10 = g[TAU2COL[0.10]].to_numpy()\n",
    "        q90 = g[TAU2COL[0.90]].to_numpy()\n",
    "        q05 = g[TAU2COL[0.05]].to_numpy()\n",
    "        q95 = g[TAU2COL[0.95]].to_numpy()\n",
    "        cover80 = ((y_t >= q10) & (y_t <= q90)).mean()\n",
    "        cover90 = ((y_t >= q05) & (y_t <= q95)).mean()\n",
    "        width80 = (q90 - q10).mean()\n",
    "        width90 = (q95 - q05).mean()\n",
    "\n",
    "        bytok.append({\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"token\": tok,\n",
    "            \"tau\": tau,\n",
    "            \"pinball_mean\": float(loss.mean()),\n",
    "            \"pinball_se\": float(loss.std(ddof=1) / max(1, math.sqrt(len(y_t)))),\n",
    "            \"coverage80\": float(cover80),\n",
    "            \"coverage90\": float(cover90),\n",
    "            \"width80_mean\": float(width80),\n",
    "            \"width90_mean\": float(width90),\n",
    "            \"n_obs\": int(len(g))\n",
    "        })\n",
    "\n",
    "bytoken_metrics = pd.DataFrame(bytok).sort_values([\"token\",\"tau\"])\n",
    "bytoken_path = OUTDIR / \"tbl_metrics_by_token_qrf.csv\"\n",
    "bytoken_metrics.to_csv(bytoken_path, index=False)\n",
    "print(f\"Saved per-token metrics ‚Üí {bytoken_path.resolve()}\")\n",
    "\n",
    "# Quick on-screen summary (nice to paste into notes)\n",
    "display_cols = [\"tau\",\"pinball_mean\",\"pinball_se\",\"coverage80\",\"coverage80_lo\",\"coverage80_hi\",\n",
    "                \"width80_mean\",\"coverage90\",\"coverage90_lo\",\"coverage90_hi\",\"width90_mean\"]\n",
    "print(pooled_metrics[display_cols].to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc267aeb",
   "metadata": {},
   "source": [
    "# Step 1 ‚Äî Evaluation lock (notes)\n",
    "\n",
    "**Results (QRF v3).**\n",
    "\n",
    "* No quantile crossings were detected (**0 violations**), confirming the isotonic guard is working.\n",
    "* Pooled coverage: **80% = 0.792** (95% CI ‚âà \\[0.778, 0.806]), **90% = 0.873** (‚âà \\[0.861, 0.884]).\n",
    "* Mean widths: **80% = 0.319**, **90% = 0.428**.\n",
    "* Pinball loss increases smoothly from tails toward the median (table screenshot), consistent with heavier central errors.\n",
    "\n",
    "**Why this matters.**\n",
    "These numbers match my earlier summary: QRF under-covers slightly at 80% and is closer at 90%, with sharp intervals relative to coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067b222",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a372cd",
   "metadata": {},
   "source": [
    "# 2. Calibration & reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03848537",
   "metadata": {},
   "source": [
    "What I did.\n",
    "I evaluated quantile calibration by comparing the predicted quantiles to empirical hit-rates: for each œÑ, I computed \n",
    "ùëù\n",
    "^\n",
    "ùúè\n",
    "=\n",
    "ùëÉ\n",
    "(\n",
    "ùë¶\n",
    "‚â§\n",
    "ùëû\n",
    "^\n",
    "ùúè\n",
    ")\n",
    "p\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    "=P(y‚â§\n",
    "q\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    ") and plotted \n",
    "ùëù\n",
    "^\n",
    "ùúè\n",
    "p\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    " against œÑ with binomial (Wilson) CIs. I produced curves globally and by regime (using my vol_regime; when absent I use width-terciles as a proxy for risk regime). I also summarised interval coverage vs nominal for the 80% and 90% bands, with CIs, and visualised interval width distributions.\n",
    "\n",
    "Why.\n",
    "Reliability curves diagnose systematic under/over-estimation of quantiles, while coverage vs nominal validates the overall calibration of my 80% and 90% intervals. Slicing by regime shows whether mis-calibration concentrates in volatile periods, which informs where conformal offsets or weighting schemes matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e17783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_15136\\4229963903.py:68: UserWarning: Glyph 119875 (\\N{MATHEMATICAL ITALIC CAPITAL P}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_15136\\4229963903.py:69: UserWarning: Glyph 119875 (\\N{MATHEMATICAL ITALIC CAPITAL P}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(OUTDIR / \"fig_reliability_global.png\", dpi=FIG_DPI)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (WindowsPath('results/tbl_reliability_global.csv'), WindowsPath('results/tbl_reliability_by_regime.csv'), WindowsPath('results/tbl_interval_coverage.csv')) and figures to C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_15136\\4229963903.py:152: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([q90 - q10, q95 - q05], labels=[\"80% width\",\"90% width\"], showfliers=False)\n"
     ]
    }
   ],
   "source": [
    "# === Step 2 ¬∑ Calibration & reliability ========================================\n",
    "import re, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- CONFIG --------------------------------------------------------------------\n",
    "PRED_PATH = Path(\"qrf_v2_tuned_preds.csv\")   # update if needed\n",
    "OUTDIR = Path(\"results\"); OUTDIR.mkdir(exist_ok=True)\n",
    "FIG_DPI = 140\n",
    "\n",
    "# ---- Load & infer taus ---------------------------------------------------------\n",
    "pred_df = pd.read_csv(PRED_PATH, parse_dates=[\"timestamp\"])\n",
    "assert {\"token\",\"timestamp\",\"y_true\"}.issubset(pred_df.columns)\n",
    "\n",
    "def infer_tau_cols(df):\n",
    "    tau2col = {}\n",
    "    for c in df.columns:\n",
    "        # Match columns like q5, q10, q25, q50, q75, q90, q95\n",
    "        m = re.fullmatch(r\"q(\\d{1,2})\", c)\n",
    "        if m:\n",
    "            tau = int(m.group(1)) / 100.0\n",
    "            tau2col[round(tau, 2)] = c\n",
    "    expected = {0.05,0.10,0.25,0.50,0.75,0.90,0.95}\n",
    "    missing = expected - set(tau2col)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing quantiles {sorted(missing)}. Found: {sorted(tau2col)}\")\n",
    "    return dict(sorted(tau2col.items()))\n",
    "TAU2COL = infer_tau_cols(pred_df)\n",
    "TAUS = list(TAU2COL.keys())\n",
    "\n",
    "# ---- Helpers -------------------------------------------------------------------\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    if n == 0: return (np.nan, np.nan)\n",
    "    z = 1.959963984540054 if alpha==0.05 else 1.2815515655446004\n",
    "    ph = k/n\n",
    "    denom = 1 + z*z/n\n",
    "    centre = (ph + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * np.sqrt((ph*(1-ph) + z*z/(4*n))/n)\n",
    "    return (centre - half, centre + half)\n",
    "\n",
    "# ---- 1) Global reliability: P(y ‚â§ q_tau) vs tau --------------------------------\n",
    "rel_rows = []\n",
    "y = pred_df[\"y_true\"].to_numpy()\n",
    "n_global = len(pred_df)\n",
    "\n",
    "for tau in TAUS:\n",
    "    q = pred_df[TAU2COL[tau]].to_numpy()\n",
    "    hits = (y <= q)\n",
    "    ph = hits.mean()\n",
    "    lo, hi = wilson_ci(hits.sum(), len(hits))\n",
    "    rel_rows.append({\"tau\": tau, \"hit_rate\": float(ph), \"lo\": float(lo), \"hi\": float(hi), \"n\": int(len(hits))})\n",
    "\n",
    "rel_global = pd.DataFrame(rel_rows)\n",
    "rel_global_path = OUTDIR / \"tbl_reliability_global.csv\"\n",
    "rel_global.to_csv(rel_global_path, index=False)\n",
    "\n",
    "# Plot global reliability\n",
    "plt.figure(figsize=(5.2,4.2))\n",
    "plt.plot(rel_global[\"tau\"], rel_global[\"hit_rate\"], marker=\"o\")\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], linestyle=\"--\")  # ideal y=x\n",
    "# error bars\n",
    "plt.errorbar(rel_global[\"tau\"], rel_global[\"hit_rate\"],\n",
    "             yerr=[rel_global[\"hit_rate\"]-rel_global[\"lo\"], rel_global[\"hi\"]-rel_global[\"hit_rate\"]],\n",
    "             fmt=\"none\", capsize=3)\n",
    "plt.xlabel(\"Nominal quantile (œÑ)\")\n",
    "plt.ylabel(\"Empirical hit-rate  ùëÉ(y ‚â§ qÃÇœÑ)\")\n",
    "plt.title(\"Reliability curve ‚Äî Global\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_reliability_global.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 2) Reliability by regime --------------------------------------------------\n",
    "df_reg = pred_df.copy()\n",
    "if \"vol_regime\" in df_reg.columns:\n",
    "    df_reg[\"regime\"] = df_reg[\"vol_regime\"].astype(str)\n",
    "else:\n",
    "    # Fallback proxy: width-terciles of 80% band\n",
    "    width80 = df_reg[TAU2COL[0.90]] - df_reg[TAU2COL[0.10]]\n",
    "    terc = pd.qcut(width80, 3, labels=[\"narrow\",\"mid\",\"wide\"])\n",
    "    df_reg[\"regime\"] = terc.astype(str)\n",
    "\n",
    "rel_reg_rows = []\n",
    "for regime, g in df_reg.groupby(\"regime\"):\n",
    "    y_r = g[\"y_true\"].to_numpy()\n",
    "    for tau in TAUS:\n",
    "        q_r = g[TAU2COL[tau]].to_numpy()\n",
    "        hits = (y_r <= q_r)\n",
    "        ph = hits.mean()\n",
    "        lo, hi = wilson_ci(hits.sum(), len(hits))\n",
    "        rel_reg_rows.append({\"regime\": regime, \"tau\": tau, \"hit_rate\": float(ph),\n",
    "                             \"lo\": float(lo), \"hi\": float(hi), \"n\": int(len(hits))})\n",
    "\n",
    "rel_by_regime = pd.DataFrame(rel_reg_rows)\n",
    "rel_by_regime_path = OUTDIR / \"tbl_reliability_by_regime.csv\"\n",
    "rel_by_regime.to_csv(rel_by_regime_path, index=False)\n",
    "\n",
    "# Plot by regime\n",
    "plt.figure(figsize=(6.2,4.4))\n",
    "for regime, g in rel_by_regime.groupby(\"regime\"):\n",
    "    g = g.sort_values(\"tau\")\n",
    "    plt.plot(g[\"tau\"], g[\"hit_rate\"], marker=\"o\", label=str(regime))\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], linestyle=\"--\")\n",
    "plt.xlabel(\"Nominal quantile (œÑ)\")\n",
    "plt.ylabel(\"Empirical hit-rate\")\n",
    "plt.title(\"Reliability curve ‚Äî By regime\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_reliability_by_regime.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 3) Interval coverage vs nominal + widths ---------------------------------\n",
    "q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "\n",
    "cover80 = ((y >= q10) & (y <= q90))\n",
    "cover90 = ((y >= q05) & (y <= q95))\n",
    "c80, c90 = cover80.mean(), cover90.mean()\n",
    "c80_lo, c80_hi = wilson_ci(cover80.sum(), len(cover80))\n",
    "c90_lo, c90_hi = wilson_ci(cover90.sum(), len(cover90))\n",
    "w80, w90 = (q90 - q10).mean(), (q95 - q05).mean()\n",
    "\n",
    "cov_tbl = pd.DataFrame({\n",
    "    \"interval\": [\"80%\", \"90%\"],\n",
    "    \"coverage\": [float(c80), float(c90)],\n",
    "    \"lo\": [float(c80_lo), float(c90_lo)],\n",
    "    \"hi\": [float(c80_hi), float(c90_hi)],\n",
    "    \"mean_width\": [float(w80), float(w90)],\n",
    "    \"n\": [int(len(cover80)), int(len(cover90))]\n",
    "})\n",
    "cov_tbl_path = OUTDIR / \"tbl_interval_coverage.csv\"\n",
    "cov_tbl.to_csv(cov_tbl_path, index=False)\n",
    "\n",
    "# Coverage figure with error bars\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "x = np.array([0,1])\n",
    "ybar = cov_tbl[\"coverage\"].to_numpy()\n",
    "yerr = np.vstack([ybar - cov_tbl[\"lo\"].to_numpy(), cov_tbl[\"hi\"].to_numpy() - ybar])\n",
    "plt.errorbar(x, ybar, yerr=yerr, fmt=\"o\", capsize=4)\n",
    "plt.hlines([0.80, 0.90], xmin=-0.3, xmax=1.3, linestyles=[\"--\",\"--\"])\n",
    "plt.xticks(x, cov_tbl[\"interval\"])\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.ylabel(\"Empirical coverage\")\n",
    "plt.title(\"Interval coverage vs nominal\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_interval_coverage.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 4) Width distributions (boxplots) -----------------------------------------\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "plt.boxplot([q90 - q10, q95 - q05], labels=[\"80% width\",\"90% width\"], showfliers=False)\n",
    "plt.ylabel(\"Width\")\n",
    "plt.title(\"Interval width distributions\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_width_distributions.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (rel_global_path, rel_by_regime_path, cov_tbl_path),\n",
    "      \"and figures to\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58611bee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 2 ‚Äî Calibration & reliability (notes)\n",
    "\n",
    "**What the plots show.**\n",
    "\n",
    "* **Global reliability:** œÑ=0.05 and œÑ=0.10 hug y=x (good), but **œÑ=0.25 jumps to \\~0.62** and œÑ=0.50 sits \\~0.74. Upper quantiles (0.75‚Äì0.95) track y=x closely.\n",
    "* **By regime:** the **œÑ=0.25 kink persists across narrow/mid/wide** regimes, so it‚Äôs systematic, not regime-specific.\n",
    "* **Coverage vs nominal:** mirrors the above‚Äîslight under-coverage at 80%, closer at 90%.\n",
    "* **Width distributions:** 90% bands are wider (as expected) with a long right tail during volatile periods.\n",
    "\n",
    "**Diagnosis.**\n",
    "That **large upward kink at œÑ=0.25** points to a calibration bug in my residual shift rule for lower quantiles. In my QRF v3 loop I set the offset for **œÑ<0.5** using **`quantile(residuals, 1 ‚àí œÑ)`**. The correct shift is **`quantile(residuals, œÑ)`** for *all* œÑ. Using `1 ‚àí œÑ` pushes lower quantiles **too high**, inflating hit-rates for œÑ=0.25 (and, via isotonicity, also lifting q50).\n",
    "\n",
    "---\n",
    "\n",
    "## One-line fix to the conformal offsets\n",
    "\n",
    "Replace `1 - tau` with `tau` for all **lower-quantile** branches (both the regime-aware block and the generic block). Here‚Äôs a drop-in replacement for your offset section:\n",
    "\n",
    "```python\n",
    "# --- compute regime-aware Œ¥œÑ on calibration residuals (correct œÑ, not 1-œÑ) -----\n",
    "offsets = np.zeros(len(quantiles))\n",
    "median_bias = np.median(residuals[valid_mask, quantiles.index(0.50)])\n",
    "\n",
    "for qi, tau in enumerate(quantiles):\n",
    "    # winsorize within the valid set\n",
    "    res_all = winsorize_residuals(residuals[valid_mask, qi])\n",
    "\n",
    "    # tails: regime-aware split if available\n",
    "    if tau in [0.05, 0.10, 0.90, 0.95] and 'vol_regime' in df_cal.columns:\n",
    "        quiet_mask = (regime_cal == 'quiet') & valid_mask\n",
    "        vol_mask   = (regime_cal == 'volatile') & valid_mask\n",
    "\n",
    "        def qtau(arr, t=tau):\n",
    "            return np.quantile(winsorize_residuals(arr), t) if arr.size > 0 else np.quantile(res_all, t)\n",
    "\n",
    "        quiet_off = qtau(residuals[quiet_mask, qi])\n",
    "        vol_off   = qtau(residuals[vol_mask, qi])\n",
    "        wq, wv = quiet_mask.sum(), vol_mask.sum()\n",
    "        offsets[qi] = (wq * quiet_off + wv * vol_off) / (wq + wv + 1e-8)\n",
    "\n",
    "    else:\n",
    "        # generic: same rule for all œÑ\n",
    "        offsets[qi] = np.quantile(res_all, tau)\n",
    "\n",
    "# apply Œ¥œÑ and median centering\n",
    "adjusted_test = preds_test + offsets\n",
    "adjusted_test[:, quantiles.index(0.50)] += median_bias\n",
    "adjusted_test = isotonic_non_crossing(adjusted_test, quantiles)\n",
    "```\n",
    "\n",
    "**Why this is correct.**\n",
    "We want $\\mathbb{P}(y \\le \\hat{q}_\\tau + \\delta_\\tau) \\approx \\tau$. With residuals $r = y - \\hat{q}_\\tau$, the shift satisfying this is $\\delta_\\tau = Q_\\tau(r)$, not $Q_{1-\\tau}(r)$.\n",
    "\n",
    "---\n",
    "\n",
    "## What to do now\n",
    "\n",
    "1. Patch the offset code above in your QRF v3 loop (no re-fit needed; it‚Äôs a recalibration step).\n",
    "2. Re-run the evaluation + reliability cells (Steps 1‚Äì2).\n",
    "3. Send me the updated `tbl_interval_coverage.csv` and a snapshot of the **global reliability** plot.\n",
    "\n",
    "If œÑ=0.25 and œÑ=0.50 come down toward the y=x line (they should), we‚Äôll proceed to:\n",
    "\n",
    "* **Step 3:** HAC-robust **Diebold‚ÄìMariano** tests (QRF vs LQR/LightGBM) with a clean per-quantile + per-token table, and\n",
    "* **Step 4:** **Feature pruning** (fold-stability + collinearity trim) and quick re-fit check.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**What I did.**\n",
    "After inspecting reliability curves, I identified a calibration error in my conformal shift rule for lower quantiles. I had incorrectly used $Q_{1-\\tau}(r)$ instead of $Q_{\\tau}(r)$ for residuals $r = y-\\hat{q}_\\tau$. I corrected the offsets to $\\delta_\\tau = Q_{\\tau}(r)$ for all œÑ, keeping the regime-aware split on tails and the isotonic non-crossing step.\n",
    "\n",
    "**Why.**\n",
    "This ensures the adjusted quantiles satisfy $\\mathbb{P}(y \\le \\hat{q}_\\tau) \\approx \\tau$ uniformly across œÑ, preventing the inflated hit-rates previously observed around œÑ=0.25‚Äì0.50 and stabilising median calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8781c0e",
   "metadata": {},
   "source": [
    "## What I did.\n",
    "I audited the volatility regime input used for regime-aware calibration. My feature table encodes vol_regime as an integer quintile in {0,1,2,3,4}, whereas my calibration code expected string labels (‚Äúquiet‚Äù/‚Äúvolatile‚Äù). As a result, the quiet/volatile masks were empty and the tail offsets defaulted to global (or ~zero), i.e. regime-awareness was effectively off. I fixed this by mapping {0,1}‚Üíquiet, {3,4}‚Üívolatile, and {2}‚Üímid, with warm-up NAs assigned to mid. I also retained a fallback that derives regimes from a past-volatility proxy (e.g., gk_vol_36h) if vol_regime is not available.\n",
    "\n",
    "## Why.\n",
    "The purpose of regime-aware calibration is to prevent under-coverage in turbulent periods without widening bands in calm periods. Ensuring the regime signal is recognised by the calibration step is essential; otherwise offsets can be biased toward average conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342173e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51242da3",
   "metadata": {},
   "source": [
    "# Step 3: HAC-robust Diebold‚ÄìMariano + per-token heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49d9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= DM utilities (run once) =================\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "TAUS = [0.05,0.10,0.25,0.50,0.75,0.90,0.95]\n",
    "TAU2COL = {0.05:\"q5\",0.10:\"q10\",0.25:\"q25\",0.50:\"q50\",0.75:\"q75\",0.90:\"q90\",0.95:\"q95\"}\n",
    "\n",
    "def pinball_loss_vec(y, q, tau):\n",
    "    diff = y - q\n",
    "    return np.maximum(tau*diff, (tau-1)*diff)\n",
    "\n",
    "def newey_west_var(d, lag=5):\n",
    "    \"\"\"Bartlett kernel HAC variance of mean(d). Returns var(mean(d)).\"\"\"\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    d = d[np.isfinite(d)]\n",
    "    n = d.size\n",
    "    if n <= 1: \n",
    "        return np.nan\n",
    "    d = d - d.mean()\n",
    "    gamma0 = np.dot(d, d) / n\n",
    "    s = gamma0\n",
    "    for k in range(1, min(lag, n-1)+1):\n",
    "        w = 1 - k/(lag+1)\n",
    "        gamma_k = np.dot(d[k:], d[:-k]) / n\n",
    "        s += 2*w*gamma_k\n",
    "    return s / n  # variance of the sample mean\n",
    "\n",
    "def dm_test(loss1, loss2, lag=5):\n",
    "    \"\"\"Two-sided DM with NW variance on loss diff.\"\"\"\n",
    "    d = np.asarray(loss1) - np.asarray(loss2)\n",
    "    var_hat = newey_west_var(d, lag=lag)\n",
    "    if not np.isfinite(var_hat) or var_hat <= 0:\n",
    "        return np.nan, np.nan\n",
    "    dm = d.mean() / np.sqrt(var_hat)\n",
    "    # normal approx for large n\n",
    "    from math import erf, sqrt\n",
    "    p = 2 * (1 - 0.5*(1 + erf(abs(dm)/np.sqrt(2))))\n",
    "    return float(dm), float(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9677fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_dm_by_token.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tau",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "QRF_vs_LQR_win",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QRF_vs_LQR_draw",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QRF_vs_LQR_loss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QRF_vs_LGBM_win",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QRF_vs_LGBM_draw",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QRF_vs_LGBM_loss",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "5d625441-4944-40e9-b0de-ea899ef5fc3b",
       "rows": [
        [
         "0",
         "0.1",
         "6",
         "13",
         "0",
         "10",
         "9",
         "0"
        ],
        [
         "1",
         "0.25",
         "7",
         "12",
         "0",
         "12",
         "7",
         "0"
        ],
        [
         "2",
         "0.5",
         "5",
         "14",
         "0",
         "7",
         "12",
         "0"
        ],
        [
         "3",
         "0.75",
         "6",
         "13",
         "0",
         "5",
         "14",
         "0"
        ],
        [
         "4",
         "0.9",
         "5",
         "14",
         "0",
         "5",
         "14",
         "0"
        ],
        [
         "5",
         "0.95",
         "4",
         "15",
         "0",
         "16",
         "3",
         "0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau</th>\n",
       "      <th>QRF_vs_LQR_win</th>\n",
       "      <th>QRF_vs_LQR_draw</th>\n",
       "      <th>QRF_vs_LQR_loss</th>\n",
       "      <th>QRF_vs_LGBM_win</th>\n",
       "      <th>QRF_vs_LGBM_draw</th>\n",
       "      <th>QRF_vs_LGBM_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.95</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tau  QRF_vs_LQR_win  QRF_vs_LQR_draw  QRF_vs_LQR_loss  QRF_vs_LGBM_win  \\\n",
       "0  0.10               6               13                0               10   \n",
       "1  0.25               7               12                0               12   \n",
       "2  0.50               5               14                0                7   \n",
       "3  0.75               6               13                0                5   \n",
       "4  0.90               5               14                0                5   \n",
       "5  0.95               4               15                0               16   \n",
       "\n",
       "   QRF_vs_LGBM_draw  QRF_vs_LGBM_loss  \n",
       "0                 9                 0  \n",
       "1                 7                 0  \n",
       "2                12                 0  \n",
       "3                14                 0  \n",
       "4                14                 0  \n",
       "5                 3                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============== DM comparisons: per-œÑ, per-token =================\n",
    "# Update paths here:\n",
    "paths = {\n",
    "    \"QRF\":       \"qrf_v2_tuned_preds.csv\",      # your final QRF v3 preds\n",
    "    \"LQR\":       \"lqr_pred_paths_full.csv\",               # <-- update\n",
    "    \"LightGBM\":  \"lgb_extended_preds.csv\"               # <-- update\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for name, path in paths.items():\n",
    "    dfp = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
    "    # Standardise quantile column names to qXX if necessary (for all models)\n",
    "    rename_cols = {}\n",
    "    for col in dfp.columns:\n",
    "        if col == 'q5_pred':\n",
    "            rename_cols[col] = 'q5'\n",
    "        elif col.startswith('q') and col.endswith('00'):\n",
    "            rename_cols[col] = f\"{col}_pred\"\n",
    "        elif col.startswith('q') and 'pred' not in col and col != 'q5':\n",
    "            rename_cols[col] = f\"{col}_pred\"\n",
    "    dfp = dfp.rename(columns=rename_cols)\n",
    "    for q in [\"05\",\"10\",\"25\",\"50\",\"75\",\"90\",\"95\"]:\n",
    "        col_pred = f\"q{q}_pred\"\n",
    "        col = f\"q{q}\"\n",
    "        if col_pred in dfp.columns and col not in dfp.columns:\n",
    "            dfp = dfp.rename(columns={col_pred: col})\n",
    "    needed = {\"token\",\"timestamp\",\"y_true\"}.union(TAU2COL.values())\n",
    "    missing = needed - set(dfp.columns)\n",
    "    # If 'q5' is missing, fill with NaN so assertion does not fail\n",
    "    if \"q5\" in missing:\n",
    "        dfp[\"q5\"] = np.nan\n",
    "        missing = needed - set(dfp.columns)\n",
    "    assert not missing, f\"{name}: missing columns {missing}\"\n",
    "    dfs[name] = dfp[[\"token\",\"timestamp\",\"y_true\"] + list(TAU2COL.values())].copy()\n",
    "\n",
    "# Inner-join on token+timestamp so all models are aligned observation-by-observation\n",
    "base = dfs[\"QRF\"][[\"token\",\"timestamp\"]].copy()\n",
    "for name in [\"LQR\",\"LightGBM\"]:\n",
    "    base = base.merge(dfs[name][[\"token\",\"timestamp\"]], on=[\"token\",\"timestamp\"], how=\"inner\")\n",
    "\n",
    "# Build aligned frames for each model\n",
    "aligned = {}\n",
    "for name, dfp in dfs.items():\n",
    "    aligned[name] = base.merge(dfp, on=[\"token\",\"timestamp\"], how=\"left\", suffixes=(\"\",\"\"))\n",
    "\n",
    "# Compute per-token DM for every œÑ (QRF vs LQR / QRF vs LightGBM)\n",
    "rows = []\n",
    "lag = 5  # horizon-1 for 72h overlapping returns (6 bars of 12h)\n",
    "for tau in TAUS:\n",
    "    qcol = TAU2COL[tau]\n",
    "    for tok, _ in aligned[\"QRF\"].groupby(\"token\"):\n",
    "        g = {m: aligned[m][aligned[m][\"token\"]==tok] for m in aligned}\n",
    "        # Intersection rows only (should align already)\n",
    "        y = g[\"QRF\"][\"y_true\"].to_numpy()\n",
    "        mask = np.isfinite(y)\n",
    "        # losses\n",
    "        L = {}\n",
    "        for m in aligned:\n",
    "            q = g[m][qcol].to_numpy()\n",
    "            mask &= np.isfinite(q)\n",
    "        # apply mask\n",
    "        y = y[mask]\n",
    "        for m in aligned:\n",
    "            q = g[m][qcol].to_numpy()[mask]\n",
    "            L[m] = pinball_loss_vec(y, q, tau)\n",
    "\n",
    "        if len(y) < 15:  # skip tiny samples\n",
    "            continue\n",
    "\n",
    "        # DM: QRF better if DM < 0 (lower loss)\n",
    "        dm_lqr, p_lqr = dm_test(L[\"QRF\"], L[\"LQR\"], lag=lag)\n",
    "        dm_lgb, p_lgb = dm_test(L[\"QRF\"], L[\"LightGBM\"], lag=lag)\n",
    "\n",
    "        rows.append({\"token\": tok, \"tau\": tau,\n",
    "                     \"dm_qrf_vs_lqr\": dm_lqr, \"p_qrf_vs_lqr\": p_lqr,\n",
    "                     \"dm_qrf_vs_lgbm\": dm_lgb, \"p_qrf_vs_lgbm\": p_lgb,\n",
    "                     \"n\": int(len(y))})\n",
    "\n",
    "dm_by_token = pd.DataFrame(rows).sort_values([\"tau\",\"token\"])\n",
    "dm_by_token.to_csv(RESULTS_DIR/\"tbl_dm_by_token.csv\", index=False)\n",
    "print(\"Saved ‚Üí\", (RESULTS_DIR/\"tbl_dm_by_token.csv\").resolve())\n",
    "\n",
    "# Win/Draw/Loss counts per œÑ (Œ± = 0.05)\n",
    "summ = []\n",
    "alpha = 0.05\n",
    "for tau, g in dm_by_token.groupby(\"tau\"):\n",
    "    def wdl(dm, p):\n",
    "        if not np.isfinite(dm) or not np.isfinite(p):\n",
    "            return \"draw\"\n",
    "        if p < alpha and dm < 0:  # QRF has lower loss\n",
    "            return \"win\"\n",
    "        if p < alpha and dm > 0:\n",
    "            return \"loss\"\n",
    "        return \"draw\"\n",
    "    wdl_lqr  = g.apply(lambda r: wdl(r[\"dm_qrf_vs_lqr\"],  r[\"p_qrf_vs_lqr\"]),  axis=1).value_counts()\n",
    "    wdl_lgbm = g.apply(lambda r: wdl(r[\"dm_qrf_vs_lgbm\"], r[\"p_qrf_vs_lgbm\"]), axis=1).value_counts()\n",
    "    summ.append({\n",
    "        \"tau\": tau,\n",
    "        \"QRF_vs_LQR_win\":  int(wdl_lqr.get(\"win\",0)),\n",
    "        \"QRF_vs_LQR_draw\": int(wdl_lqr.get(\"draw\",0)),\n",
    "        \"QRF_vs_LQR_loss\": int(wdl_lqr.get(\"loss\",0)),\n",
    "        \"QRF_vs_LGBM_win\":  int(wdl_lgbm.get(\"win\",0)),\n",
    "        \"QRF_vs_LGBM_draw\": int(wdl_lgbm.get(\"draw\",0)),\n",
    "        \"QRF_vs_LGBM_loss\": int(wdl_lgbm.get(\"loss\",0)),\n",
    "    })\n",
    "dm_counts = pd.DataFrame(summ).sort_values(\"tau\")\n",
    "dm_counts.to_csv(RESULTS_DIR/\"tbl_dm_counts.csv\", index=False)\n",
    "dm_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ed92a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heatmap ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\fig_dm_heatmap_qrf_vs_lgbm.png\n"
     ]
    }
   ],
   "source": [
    "# ============== Heatmap of DM statistics (QRF vs LightGBM) =====================\n",
    "pivot = dm_by_token.pivot(index=\"token\", columns=\"tau\", values=\"dm_qrf_vs_lgbm\")\n",
    "plt.figure(figsize=(8, max(4, 0.35*len(pivot))))\n",
    "im = plt.imshow(pivot.values, aspect=\"auto\", cmap=\"coolwarm\", vmin=-3, vmax=3)  # clip around ¬±3œÉ\n",
    "plt.colorbar(im, label=\"DM statistic (QRF ‚Äì LGBM)\")\n",
    "plt.xticks(range(len(pivot.columns)), [f\"{t:.2f}\" for t in pivot.columns], rotation=0)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.title(\"Per-token Diebold‚ÄìMariano: QRF vs LightGBM (pinball loss)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR/\"fig_dm_heatmap_qrf_vs_lgbm.png\", dpi=160)\n",
    "plt.close()\n",
    "print(\"Saved heatmap ‚Üí\", (RESULTS_DIR/\"fig_dm_heatmap_qrf_vs_lgbm.png\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eceb24",
   "metadata": {},
   "source": [
    "### What I did.\n",
    "I compared QRF to both baselines with Diebold‚ÄìMariano tests on pinball loss for each quantile œÑ and token. Because my 72-hour target overlaps (six 12-h bars), I used a Newey‚ÄìWest HAC variance with lag 5 to account for serial correlation in the loss differences. I report (i) a per-token DM table (statistic and p-value) and (ii) win/draw/loss counts per œÑ at Œ±=0.05. I also include a DM heatmap (QRF vs LightGBM) to show where QRF‚Äôs edge concentrates across tokens and quantiles.\n",
    "\n",
    "### Why.\n",
    "Pinball loss is proper for quantiles, but sampling variability and serial dependence can blur comparisons. HAC-robust DM tests provide a principled significance check under overlapping horizons, supporting claims like ‚ÄúQRF significantly outperforms LightGBM at œÑ‚àà{0.25,‚Ä¶} across most tokens.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcfb23",
   "metadata": {},
   "source": [
    "## Diebold‚ÄìMariano\n",
    "\n",
    "* **Across tokens and quantiles, QRF generally wins.** The DM heatmap (QRF‚ÄìLGBM) is predominantly **blue** from œÑ=0.10 through œÑ=0.95, indicating **lower pinball loss** for QRF across the panel.\n",
    "* **Local exceptions:** POPCAT at **œÑ‚âà0.90** shows **positive DM** (LGBM lower loss), and GOAT at **œÑ‚âà0.50** also tilts toward LGBM. These are plausible where tails are very asymmetric or where on-chain features are heavily imputed.\n",
    "* **Extremes behave sensibly:** At **œÑ=0.95** you still see many dark blues (QRF wins), suggesting QRF‚Äôs calibrated upper tail remains sharper without drifting into under-coverage.\n",
    "* **Takeaway:** QRF‚Äôs edge is **broad-based**, not concentrated in a single œÑ or a single token. The few red patches highlight candidates for token-level diagnostics (missingness, regime mix) and justify the token-filtering sanity check we queued up.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2cc9d",
   "metadata": {},
   "source": [
    "# Model Confidence Set (MCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf916a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilites\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "TAUS = [0.05,0.10,0.25,0.50,0.75,0.90,0.95]\n",
    "TAU2COL = {0.05:\"q5\",0.10:\"q10\",0.25:\"q25\",0.50:\"q50\",0.75:\"q75\",0.90:\"q90\",0.95:\"q95\"}\n",
    "\n",
    "def pinball_loss_vec(y, q, tau):\n",
    "    diff = y - q\n",
    "    return np.maximum(tau*diff, (tau-1)*diff)\n",
    "\n",
    "def moving_block_bootstrap_indices(n, block_len, rng):\n",
    "    \"\"\"Return indices for one bootstrap sample of length n using moving blocks.\"\"\"\n",
    "    if n <= block_len:\n",
    "        start = rng.integers(0, max(1, n-1))\n",
    "        idx = np.arange(start, min(n, start+block_len))\n",
    "        return np.resize(idx, n)\n",
    "    starts = rng.integers(0, n - block_len + 1, size=int(np.ceil(n / block_len)))\n",
    "    idx = np.concatenate([np.arange(s, s+block_len) for s in starts])[:n]\n",
    "    return idx\n",
    "\n",
    "def tokenwise_block_resample(panel, block_len, rng):\n",
    "    \"\"\"Resample *within each token* to preserve each token's serial dependence.\"\"\"\n",
    "    out = []\n",
    "    for tok, g in panel.groupby(\"token\", sort=False):\n",
    "        idx = moving_block_bootstrap_indices(len(g), block_len, rng)\n",
    "        out.append(g.iloc[idx])\n",
    "    return pd.concat(out, axis=0, ignore_index=True)\n",
    "\n",
    "def build_aligned_panel(paths):\n",
    "    \"\"\"Return a long panel: columns [token,timestamp,model,tau,loss].\"\"\"\n",
    "    dfs = {}\n",
    "    for name, path in paths.items():\n",
    "        dfp = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
    "        # Standardise quantile column names to qXX if necessary (for all models)\n",
    "        rename_cols = {}\n",
    "        for col in dfp.columns:\n",
    "            if col == 'q5_pred':\n",
    "                rename_cols[col] = 'q5'\n",
    "            elif col.startswith('q') and col.endswith('00'):\n",
    "                rename_cols[col] = f\"{col}_pred\"\n",
    "            elif col.startswith('q') and 'pred' not in col and col != 'q5':\n",
    "                rename_cols[col] = f\"{col}_pred\"\n",
    "        dfp = dfp.rename(columns=rename_cols)\n",
    "        for q in [\"05\",\"10\",\"25\",\"50\",\"75\",\"90\",\"95\"]:\n",
    "            col_pred = f\"q{q}_pred\"\n",
    "            col = f\"q{q}\"\n",
    "            if col_pred in dfp.columns and col not in dfp.columns:\n",
    "                dfp = dfp.rename(columns={col_pred: col})\n",
    "        needed = {\"token\",\"timestamp\",\"y_true\"}.union(TAU2COL.values())\n",
    "        missing = needed - set(dfp.columns)\n",
    "        # If 'q5' is missing, fill with NaN so assertion does not fail\n",
    "        if \"q5\" in missing:\n",
    "            dfp[\"q5\"] = np.nan\n",
    "            missing = needed - set(dfp.columns)\n",
    "        assert not missing, f\"{name}: missing columns {missing}\"\n",
    "        dfs[name] = dfp[[\"token\",\"timestamp\",\"y_true\"] + list(TAU2COL.values())].copy()\n",
    "\n",
    "    # Align on the intersection of timestamps per token across all models\n",
    "    base = dfs[next(iter(dfs))][[\"token\",\"timestamp\"]].copy()\n",
    "    for name in dfs:\n",
    "        if name == next(iter(dfs)): \n",
    "            continue\n",
    "        base = base.merge(dfs[name][[\"token\",\"timestamp\"]], on=[\"token\",\"timestamp\"], how=\"inner\")\n",
    "\n",
    "    panels = []\n",
    "    for name, dfp in dfs.items():\n",
    "        g = base.merge(dfp, on=[\"token\",\"timestamp\"], how=\"left\")\n",
    "        long = []\n",
    "        for tau, qcol in TAU2COL.items():\n",
    "            loss = pinball_loss_vec(g[\"y_true\"].to_numpy(), g[qcol].to_numpy(), tau)\n",
    "            long.append(pd.DataFrame({\n",
    "                \"token\": g[\"token\"].values,\n",
    "                \"timestamp\": g[\"timestamp\"].values,\n",
    "                \"model\": name,\n",
    "                \"tau\": tau,\n",
    "                \"loss\": loss\n",
    "            }))\n",
    "        panels.append(pd.concat(long, axis=0, ignore_index=True))\n",
    "    panel = pd.concat(panels, axis=0, ignore_index=True)\n",
    "    # keep finite rows only\n",
    "    panel = panel[np.isfinite(panel[\"loss\"])].reset_index(drop=True)\n",
    "    return panel\n",
    "\n",
    "def mcs_once(loss_mat, models, rng, block_len=6, B=1000, alpha=0.10):\n",
    "    \"\"\"\n",
    "    Hansen et al. MCS using the Tmax statistic:\n",
    "    - d_i,t = l_i,t - mean_j l_j,t\n",
    "    - t_i = sqrt(n)*mean(d_i)/sd_bootstrap(mean(d_i)^*)\n",
    "    - T_max = max_i t_i; eliminate argmax if p < alpha\n",
    "    Returns surviving models and elimination log.\n",
    "    \"\"\"\n",
    "    current = list(models)\n",
    "    elim_log = []\n",
    "    # loss_mat: dataframe with columns ['token','timestamp'] + models, for a fixed œÑ\n",
    "    base_cols = [\"token\",\"timestamp\"]\n",
    "    key = loss_mat[base_cols].copy()\n",
    "\n",
    "    while len(current) > 1:\n",
    "        L = loss_mat[current].to_numpy()\n",
    "        n = L.shape[0]\n",
    "        # d_i,t relative to cross-model mean\n",
    "        d = L - L.mean(axis=1, keepdims=True)  # (n, m)\n",
    "        dbar = d.mean(axis=0)                  # (m,)\n",
    "        # bootstrap means of d_i\n",
    "        dbar_boot = []\n",
    "        for b in range(B):\n",
    "            # resample tokenwise with blocks\n",
    "            boot_idx = []\n",
    "            for tok, g in loss_mat.groupby(\"token\", sort=False):\n",
    "                idx = moving_block_bootstrap_indices(len(g), block_len, rng)\n",
    "                # Map to the corresponding rows of this tau-specific matrix\n",
    "                start = g.index.min()\n",
    "                boot_idx.append(start + idx)\n",
    "            boot_idx = np.concatenate(boot_idx)\n",
    "            db = d[boot_idx, :].mean(axis=0)\n",
    "            dbar_boot.append(db)\n",
    "        dbar_boot = np.vstack(dbar_boot)  # (B, m)\n",
    "        # studentized t_i\n",
    "        sd = dbar_boot.std(axis=0, ddof=1)\n",
    "        # avoid zeros\n",
    "        sd = np.where(sd <= 1e-12, np.inf, sd)\n",
    "        t_i = np.sqrt(n) * dbar / sd\n",
    "        T_obs = np.max(t_i)\n",
    "\n",
    "        # bootstrap Tmax\n",
    "        t_i_boot = np.sqrt(n) * (dbar_boot - dbar) / sd\n",
    "        T_boot = np.max(t_i_boot, axis=1)\n",
    "        pval = float((T_boot >= T_obs).mean())\n",
    "\n",
    "        # stop if we can't reject EPA\n",
    "        if pval >= alpha:\n",
    "            break\n",
    "\n",
    "        # eliminate worst model (largest t_i)\n",
    "        worst_idx = int(np.argmax(t_i))\n",
    "        worst_model = current[worst_idx]\n",
    "        elim_log.append({\"eliminated\": worst_model, \"Tmax\": float(T_obs), \"pval\": pval, \"k\": len(current)})\n",
    "        current.pop(worst_idx)\n",
    "        # drop the model from loss_mat\n",
    "        loss_mat = loss_mat.drop(columns=[worst_model])\n",
    "\n",
    "    return current, pd.DataFrame(elim_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8d396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_mcs_survivors.csv C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_mcs_elimination_log.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tau",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "survivors",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "233f135e-e3eb-4bc7-8bcd-fb5b5503a271",
       "rows": [
        [
         "0",
         "0.05",
         "insufficient data"
        ],
        [
         "1",
         "0.1",
         "QRF"
        ],
        [
         "2",
         "0.25",
         "QRF"
        ],
        [
         "3",
         "0.5",
         "QRF"
        ],
        [
         "4",
         "0.75",
         "QRF"
        ],
        [
         "5",
         "0.9",
         "QRF,LQR,LightGBM"
        ],
        [
         "6",
         "0.95",
         "QRF,LQR"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau</th>\n",
       "      <th>survivors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>insufficient data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>QRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>QRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>QRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75</td>\n",
       "      <td>QRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.90</td>\n",
       "      <td>QRF,LQR,LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.95</td>\n",
       "      <td>QRF,LQR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tau          survivors\n",
       "0  0.05  insufficient data\n",
       "1  0.10                QRF\n",
       "2  0.25                QRF\n",
       "3  0.50                QRF\n",
       "4  0.75                QRF\n",
       "5  0.90   QRF,LQR,LightGBM\n",
       "6  0.95            QRF,LQR"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run MCS across œÑ (pooled over tokens)\n",
    "\n",
    "# Set your file paths here\n",
    "paths = {\n",
    "    \"QRF\":      \"qrf_v2_tuned_preds.csv\",\n",
    "    \"LQR\":      \"lqr_pred_paths_full.csv\",     # <-- update to your path\n",
    "    \"LightGBM\": \"lgb_extended_preds.csv\"     # <-- update to your path\n",
    "}\n",
    "\n",
    "panel = build_aligned_panel(paths)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "alpha = 0.10\n",
    "B = 1000\n",
    "block_len = 6  # 6√ó12h = 72h overlap\n",
    "\n",
    "survivors, logs = [], []\n",
    "\n",
    "for tau in TAUS:\n",
    "    sub = panel[panel[\"tau\"] == tau].copy()\n",
    "    if sub.empty:\n",
    "        survivors.append({\"tau\": tau, \"survivors\": \"no data\"})\n",
    "        continue\n",
    "\n",
    "    sub = sub.sort_values([\"token\",\"timestamp\",\"model\"])\n",
    "    pivot = sub.pivot_table(index=[\"token\",\"timestamp\"], columns=\"model\", values=\"loss\", aggfunc=\"mean\")\n",
    "\n",
    "    # Ensure all model columns exist; if absent, create filled with NaN\n",
    "    for m in paths.keys():\n",
    "        if m not in pivot.columns:\n",
    "            pivot[m] = np.nan\n",
    "\n",
    "    pivot = pivot.reset_index()\n",
    "\n",
    "    # Keep only models actually present as columns\n",
    "    present_models = [m for m in paths.keys() if m in pivot.columns]\n",
    "    if len(present_models) < 2:\n",
    "        survivors.append({\"tau\": tau, \"survivors\": \"insufficient models\"})\n",
    "        continue\n",
    "\n",
    "    # Drop rows with NaN in any of the present models (so comparisons are aligned)\n",
    "    pivot = pivot.dropna(subset=present_models)\n",
    "    if pivot.empty or pivot.shape[0] < 20:\n",
    "        survivors.append({\"tau\": tau, \"survivors\": \"insufficient data\"})\n",
    "        continue\n",
    "\n",
    "    # If more than 1 model present, run MCS on those\n",
    "    keep, log = mcs_once(\n",
    "        loss_mat=pivot[[\"token\",\"timestamp\"] + present_models],\n",
    "        models=present_models,\n",
    "        rng=rng, block_len=block_len, B=B, alpha=alpha\n",
    "    )\n",
    "\n",
    "    survivors.append({\"tau\": tau, \"survivors\": \",\".join(keep)})\n",
    "    if len(log):\n",
    "        log[\"tau\"] = tau\n",
    "        logs.append(log)\n",
    "\n",
    "mcs_survivors = pd.DataFrame(survivors)\n",
    "mcs_log = pd.concat(logs, ignore_index=True) if len(logs) else pd.DataFrame(columns=[\"eliminated\",\"Tmax\",\"pval\",\"k\",\"tau\"])\n",
    "\n",
    "mcs_survivors.to_csv(RESULTS_DIR/\"tbl_mcs_survivors.csv\", index=False)\n",
    "mcs_log.to_csv(RESULTS_DIR/\"tbl_mcs_elimination_log.csv\", index=False)\n",
    "print(\"Saved:\", (RESULTS_DIR/\"tbl_mcs_survivors.csv\").resolve(), (RESULTS_DIR/\"tbl_mcs_elimination_log.csv\").resolve())\n",
    "mcs_survivors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dc17d",
   "metadata": {},
   "source": [
    "## What I did.\n",
    "I applied the Model Confidence Set (MCS) procedure to pinball loss for each quantile œÑ, pooling observations across tokens while preserving within-token serial dependence via a moving-block bootstrap (block length 6). Starting from the full model set {QRF, LQR, LightGBM}, I iteratively removed the model with the largest Tmax statistic when the null of Equal Predictive Ability could be rejected at Œ±=0.10, until a final ‚Äúconfidence set‚Äù remained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624571d",
   "metadata": {},
   "source": [
    "Using the Hansen et al. MCS at Œ±=0.10 with a moving-block bootstrap (block length 6 to reflect the overlapping 72-hour horizon), QRF is retained as the sole member of the confidence set at œÑ‚àà{0.10, 0.25, 0.50, 0.75}. At œÑ=0.95, the set contains {QRF, LQR}, while at œÑ=0.90 the MCS retains all three models (EPA cannot be rejected). These outcomes align with the DM heatmap: QRF‚Äôs advantage is broad-based across central and upper quantiles, whereas at œÑ=0.90‚Äîa region sensitive to volatility bursts‚Äîdifferences become statistically indistinguishable after accounting for serial dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc71d3",
   "metadata": {},
   "source": [
    "At Œ±=0.10, the model confidence set contains QRF alone at œÑ‚àà{0.10, 0.25, 0.50, 0.75}; at œÑ=0.95 it retains {QRF, LQR}, and at œÑ=0.90 it retains all three models, indicating that equal predictive ability cannot be rejected at that quantile. These outcomes mirror the DM analysis and support QRF as the dominant procedure across most of the quantile grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9695dc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db718cb",
   "metadata": {},
   "source": [
    "# Step 4: Feature pruning\n",
    "Permutation importance per fold (QRF v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160d057f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# --------- compute permutation importance across rolling folds -----------------\u001b[39;00m\n\u001b[32m     56\u001b[39m imp_rows = []\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m     59\u001b[39m     df_tok = df[df[\u001b[33m'\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m'\u001b[39m] == token].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     60\u001b[39m     n, start, fold_idx = \u001b[38;5;28mlen\u001b[39m(df_tok), \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# ------- config for importance -----------------------------------------------\n",
    "tau_eval = [0.10, 0.50, 0.90]   # aggregate pinball across these œÑ (robust & fast)\n",
    "n_repeats = 3                   # repeats per feature for stability\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def _agg_pinball(y_true, Q, taus=tau_eval):\n",
    "    \"\"\"Aggregate pinball loss across selected taus from (n, len(quantiles)) array Q.\"\"\"\n",
    "    loss = 0.0\n",
    "    for t in taus:\n",
    "        qi = quantiles.index(t)\n",
    "        loss += mean_pinball_loss(y_true, Q[:, qi], alpha=t)\n",
    "    return loss / len(taus)\n",
    "\n",
    "def predict_adjusted(pipe, X_cal, y_cal, X_test):\n",
    "    \"\"\"Predict quantiles then apply residual offsets + split-conformal + isotonic (your v3 logic).\"\"\"\n",
    "    preds_cal  = np.array(pipe.predict(X_cal,  quantiles=quantiles))\n",
    "    preds_test = np.array(pipe.predict(X_test, quantiles=quantiles))\n",
    "    residuals  = y_cal.values.reshape(-1, 1) - preds_cal\n",
    "\n",
    "    cal_mask = make_cal_mask(X_cal.join(y_cal), y_cal, imputation_mask_cols)\n",
    "    regime_labels = resolve_regime_labels(X_cal.join(y_cal))\n",
    "\n",
    "    # residual quantile offsets (NaN-safe)\n",
    "    offsets = np.zeros(len(quantiles), dtype=float)\n",
    "    for qi, tau in enumerate(quantiles):\n",
    "        res_all = winsorize_residuals_nan(residuals[cal_mask, qi])\n",
    "        if tau in (0.05, 0.10, 0.90, 0.95):\n",
    "            qmask = ((regime_labels == \"quiet\").to_numpy()) & cal_mask\n",
    "            vmask = ((regime_labels == \"volatile\").to_numpy()) & cal_mask\n",
    "            qres = winsorize_residuals_nan(residuals[qmask, qi])\n",
    "            vres = winsorize_residuals_nan(residuals[vmask, qi])\n",
    "            wq, wv = qres.size, vres.size\n",
    "            if (wq + wv) == 0:\n",
    "                offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "            else:\n",
    "                q_off = nanquant(qres, tau, fallback=nanquant(res_all, tau))\n",
    "                v_off = nanquant(vres, tau, fallback=nanquant(res_all, tau))\n",
    "                offsets[qi] = (wq*q_off + wv*v_off) / (wq + wv)\n",
    "        else:\n",
    "            offsets[qi] = nanquant(res_all, tau, fallback=0.0)\n",
    "\n",
    "    adj_cal  = isotonic_non_crossing(preds_cal  + offsets, quantiles)\n",
    "    adj_test = isotonic_non_crossing(preds_test + offsets, quantiles)\n",
    "\n",
    "    # split-conformal widening (two-sided)\n",
    "    i05 = quantiles.index(0.05); i10 = quantiles.index(0.10)\n",
    "    i90 = quantiles.index(0.90); i95 = quantiles.index(0.95)\n",
    "    delta80 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i10], adj_cal[:, i90], coverage=0.80)\n",
    "    delta90 = split_conformal_delta_two_sided(y_cal.values, adj_cal[:, i05], adj_cal[:, i95], coverage=0.90)\n",
    "    adj_test[:, i10] -= delta80; adj_test[:, i90] += delta80\n",
    "    adj_test[:, i05] -= delta90; adj_test[:, i95] += delta90\n",
    "    adj_test = isotonic_non_crossing(adj_test, quantiles)\n",
    "    return adj_test\n",
    "\n",
    "# --------- compute permutation importance across rolling folds -----------------\n",
    "imp_rows = []\n",
    "\n",
    "for token in df['token'].unique():\n",
    "    df_tok = df[df['token'] == token].reset_index(drop=True)\n",
    "    n, start, fold_idx = len(df_tok), 0, 0\n",
    "\n",
    "    while start + train_len + cal_len + test_len <= n:\n",
    "        tr = slice(start, start + train_len)\n",
    "        ca = slice(start + train_len, start + train_len + cal_len)\n",
    "        te = slice(start + train_len + cal_len, start + train_len + cal_len + test_len)\n",
    "\n",
    "        df_train, df_cal, df_test = df_tok.iloc[tr], df_tok.iloc[ca], df_tok.iloc[te]\n",
    "        X_train, y_train = df_train[feature_cols], df_train[target_col]\n",
    "        X_cal,   y_cal   = df_cal[feature_cols],   df_cal[target_col]\n",
    "        X_test,  y_test  = df_test[feature_cols],  df_test[target_col]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('qrf', RandomForestQuantileRegressor(\n",
    "                n_estimators=best_params.get(\"n_estimators\", 1000),\n",
    "                min_samples_leaf=best_params.get(\"min_samples_leaf\", 10),\n",
    "                max_features=best_params.get(\"max_features\", \"sqrt\"),\n",
    "                max_depth=best_params.get(\"max_depth\", None),\n",
    "                bootstrap=True, random_state=42, n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train, qrf__sample_weight=compute_decay_weights(len(y_train), 60))\n",
    "\n",
    "        # baseline adjusted test predictions & aggregate loss\n",
    "        adj_test_base = predict_adjusted(pipe, X_cal, y_cal, X_test)\n",
    "        base_loss = _agg_pinball(y_test.values, adj_test_base)\n",
    "\n",
    "        # permutation per feature\n",
    "        for f in feature_cols:\n",
    "            imp_vals = []\n",
    "            for _ in range(n_repeats):\n",
    "                Xp = X_test.copy()\n",
    "                Xp[f] = rng.permutation(Xp[f].values)\n",
    "                adj_test_perm = predict_adjusted(pipe, X_cal, y_cal, Xp)\n",
    "                loss_p = _agg_pinball(y_test.values, adj_test_perm)\n",
    "                imp_vals.append(loss_p - base_loss)  # increase in loss\n",
    "            imp_rows.append({\n",
    "                \"token\": token, \"fold\": fold_idx, \"feature\": f,\n",
    "                \"importance\": float(np.median(imp_vals))\n",
    "            })\n",
    "\n",
    "        start += step; fold_idx += 1\n",
    "\n",
    "imp_df = pd.DataFrame(imp_rows)\n",
    "imp_df.to_csv(\"results/imp_qrf_by_fold.csv\", index=False)\n",
    "imp_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
