{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727a9259",
   "metadata": {},
   "source": [
    "# QRF V4: lock evaluation + bug guards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea605cc",
   "metadata": {},
   "source": [
    "I implemented a single evaluation harness that reads my QRF v3 predictions and computes pooled (all tokens) and per-token metrics across œÑ ‚àà {0.05,‚Ä¶,0.95}. It hard-checks two invariants: (i) pinball loss is non-negative, and (ii) quantiles do not cross (q05 ‚â§ q10 ‚â§ ‚Ä¶ ‚â§ q95). If any crossings slip through, I apply a monotonicity fix (cumulative max) purely for reporting. For intervals, I report empirical coverage (80%: q10‚Äìq90; 90%: q05‚Äìq95) with Wilson binomial CIs, and mean interval widths. I export two Quarto-ready tables: tbl_metrics_by_tau_qrf.csv (pooled by œÑ) and tbl_metrics_by_token_qrf.csv (per token √ó œÑ).\n",
    "\n",
    "Why.\n",
    "This locks a trustworthy baseline for all subsequent comparisons and figures (calibration, significance, sharpness). The non-crossing + non-negativity checks prevent silent bugs from contaminating calibration and DM tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16345c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-crossing violations: 0\n",
      "Saved pooled metrics ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_metrics_by_tau_qrf.csv\n",
      "Saved per-token metrics ‚Üí C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\\tbl_metrics_by_token_qrf.csv\n",
      "   tau  pinball_mean  pinball_se  coverage80  coverage80_lo  coverage80_hi  width80_mean  coverage90  coverage90_lo  coverage90_hi  width90_mean\n",
      "0.0500        0.0142      0.0008      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.1000        0.0229      0.0014      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.2500        0.0420      0.0033      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.5000        0.0653      0.0064      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.7500        0.0689      0.0090      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.9000        0.0670      0.0098      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n",
      "0.9500        0.0520      0.0089      0.6793         0.6630         0.6951        0.3827      0.7667         0.7519         0.7809        0.4820\n"
     ]
    }
   ],
   "source": [
    "# === Step 1 ¬∑ Evaluation harness + sanity guards =================================\n",
    "import re, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- CONFIG --------------------------------------------------------------------\n",
    "PRED_PATH = Path(\"qrf_v2_tuned_preds.csv\")   # update if needed\n",
    "MODEL_NAME = \"QRF_v3\"\n",
    "OUTDIR = Path(\"results\"); OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- 1) Load predictions -------------------------------------------------------\n",
    "pred_df = pd.read_csv(PRED_PATH, parse_dates=[\"timestamp\"])\n",
    "assert {\"token\",\"timestamp\",\"y_true\"}.issubset(pred_df.columns), \"pred_df must have token, timestamp, y_true\"\n",
    "\n",
    "# ---- 2) Infer quantile columns (expects q5,q10,q25,q50,q75,q90,q95) ------------\n",
    "def infer_tau_cols(df):\n",
    "    tau2col = {}\n",
    "    for c in df.columns:\n",
    "        m = re.fullmatch(r\"q(\\d{1,2})\", c)  # q5, q10, q25, ...\n",
    "        if m:\n",
    "            tau = int(m.group(1)) / 100.0\n",
    "            tau2col[round(tau, 2)] = c\n",
    "    if not {0.05,0.10,0.25,0.50,0.75,0.90,0.95}.issubset(tau2col):\n",
    "        raise ValueError(f\"Missing expected quantile columns. Found: {sorted(tau2col.items())}\")\n",
    "    return dict(sorted(tau2col.items()))\n",
    "TAU2COL = infer_tau_cols(pred_df)\n",
    "TAUS = list(TAU2COL.keys())\n",
    "\n",
    "# ---- 3) Sanity: quantile non-crossing check -----------------------------------\n",
    "def count_crossings(row, taus=TAUS, tau2col=TAU2COL):\n",
    "    vals = [row[tau2col[t]] for t in taus]\n",
    "    return np.sum(np.diff(vals) < -1e-12)\n",
    "\n",
    "cross_viol = pred_df.apply(count_crossings, axis=1).sum()\n",
    "print(f\"Non-crossing violations: {cross_viol:,}\")\n",
    "\n",
    "# Optional quick-fix (monotone enforce): cumulative max over taus\n",
    "# (You already do isotonic during inference; this is a belt-and-braces guard.)\n",
    "if cross_viol > 0:\n",
    "    qcols = [TAU2COL[t] for t in TAUS]\n",
    "    Q = pred_df[qcols].to_numpy()\n",
    "    Q_fix = np.maximum.accumulate(Q, axis=1)\n",
    "    pred_df[qcols] = Q_fix\n",
    "    cross_viol_after = pred_df.apply(count_crossings, axis=1).sum()\n",
    "    print(f\"After monotone fix, violations: {cross_viol_after:,}\")\n",
    "\n",
    "# ---- 4) Pinball loss utilities -------------------------------------------------\n",
    "def pinball_loss_vec(y, q, tau):\n",
    "    # proper quantile loss, vectorised\n",
    "    diff = y - q\n",
    "    return np.maximum(tau*diff, (tau-1)*diff)\n",
    "\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    if n == 0: \n",
    "        return (np.nan, np.nan)\n",
    "    from math import sqrt\n",
    "    z = 1.959963984540054 if alpha==0.05 else 1.2815515655446004  # 95% default\n",
    "    phat = k/n\n",
    "    denom = 1 + z**2/n\n",
    "    centre = (phat + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * sqrt((phat*(1-phat) + z*z/(4*n)) / n)\n",
    "    return (centre - half, centre + half)\n",
    "\n",
    "# ---- 5) Compute pooled metrics by œÑ --------------------------------------------\n",
    "rows = []\n",
    "y = pred_df[\"y_true\"].to_numpy()\n",
    "for tau in TAUS:\n",
    "    col = TAU2COL[tau]\n",
    "    q = pred_df[col].to_numpy()\n",
    "\n",
    "    loss = pinball_loss_vec(y, q, tau)\n",
    "    # assert non-negativity up to tiny fp tolerance\n",
    "    assert (loss >= -1e-12).all(), f\"Negative pinball detected at tau={tau}; check your pipeline.\"\n",
    "\n",
    "    # widths & coverage for 80% and 90% intervals\n",
    "    q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "    q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "    q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "    q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "    cover80_mask = (y >= q10) & (y <= q90)\n",
    "    cover90_mask = (y >= q05) & (y <= q95)\n",
    "\n",
    "    n = len(y)\n",
    "    c80 = cover80_mask.mean()\n",
    "    c90 = cover90_mask.mean()\n",
    "    c80_lo, c80_hi = wilson_ci(cover80_mask.sum(), n)\n",
    "    c90_lo, c90_hi = wilson_ci(cover90_mask.sum(), n)\n",
    "\n",
    "    width80 = (q90 - q10).mean()\n",
    "    width90 = (q95 - q05).mean()\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tau\": tau,\n",
    "        \"pinball_mean\": float(loss.mean()),\n",
    "        \"pinball_se\": float(loss.std(ddof=1) / math.sqrt(n)),\n",
    "        \"coverage80\": float(c80),\n",
    "        \"coverage80_lo\": float(c80_lo),\n",
    "        \"coverage80_hi\": float(c80_hi),\n",
    "        \"width80_mean\": float(width80),\n",
    "        \"coverage90\": float(c90),\n",
    "        \"coverage90_lo\": float(c90_lo),\n",
    "        \"coverage90_hi\": float(c90_hi),\n",
    "        \"width90_mean\": float(width90),\n",
    "        \"n_obs\": int(n)\n",
    "    })\n",
    "\n",
    "pooled_metrics = pd.DataFrame(rows).sort_values([\"tau\"])\n",
    "pooled_path = OUTDIR / \"tbl_metrics_by_tau_qrf.csv\"\n",
    "pooled_metrics.to_csv(pooled_path, index=False)\n",
    "print(f\"Saved pooled metrics ‚Üí {pooled_path.resolve()}\")\n",
    "\n",
    "# ---- 6) Per-token metrics (for appendix & DM later) ----------------------------\n",
    "bytok = []\n",
    "for (tok), g in pred_df.groupby(\"token\", sort=False):\n",
    "    y_t = g[\"y_true\"].to_numpy()\n",
    "    for tau in TAUS:\n",
    "        q_t = g[TAU2COL[tau]].to_numpy()\n",
    "        loss = pinball_loss_vec(y_t, q_t, tau)\n",
    "        assert (loss >= -1e-12).all(), f\"Negative pinball for token={tok}, tau={tau}\"\n",
    "\n",
    "        q10 = g[TAU2COL[0.10]].to_numpy()\n",
    "        q90 = g[TAU2COL[0.90]].to_numpy()\n",
    "        q05 = g[TAU2COL[0.05]].to_numpy()\n",
    "        q95 = g[TAU2COL[0.95]].to_numpy()\n",
    "        cover80 = ((y_t >= q10) & (y_t <= q90)).mean()\n",
    "        cover90 = ((y_t >= q05) & (y_t <= q95)).mean()\n",
    "        width80 = (q90 - q10).mean()\n",
    "        width90 = (q95 - q05).mean()\n",
    "\n",
    "        bytok.append({\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"token\": tok,\n",
    "            \"tau\": tau,\n",
    "            \"pinball_mean\": float(loss.mean()),\n",
    "            \"pinball_se\": float(loss.std(ddof=1) / max(1, math.sqrt(len(y_t)))),\n",
    "            \"coverage80\": float(cover80),\n",
    "            \"coverage90\": float(cover90),\n",
    "            \"width80_mean\": float(width80),\n",
    "            \"width90_mean\": float(width90),\n",
    "            \"n_obs\": int(len(g))\n",
    "        })\n",
    "\n",
    "bytoken_metrics = pd.DataFrame(bytok).sort_values([\"token\",\"tau\"])\n",
    "bytoken_path = OUTDIR / \"tbl_metrics_by_token_qrf.csv\"\n",
    "bytoken_metrics.to_csv(bytoken_path, index=False)\n",
    "print(f\"Saved per-token metrics ‚Üí {bytoken_path.resolve()}\")\n",
    "\n",
    "# Quick on-screen summary (nice to paste into notes)\n",
    "display_cols = [\"tau\",\"pinball_mean\",\"pinball_se\",\"coverage80\",\"coverage80_lo\",\"coverage80_hi\",\n",
    "                \"width80_mean\",\"coverage90\",\"coverage90_lo\",\"coverage90_hi\",\"width90_mean\"]\n",
    "print(pooled_metrics[display_cols].to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc267aeb",
   "metadata": {},
   "source": [
    "# Step 1 ‚Äî Evaluation lock (notes)\n",
    "\n",
    "**Results (QRF v3).**\n",
    "\n",
    "* No quantile crossings were detected (**0 violations**), confirming the isotonic guard is working.\n",
    "* Pooled coverage: **80% = 0.792** (95% CI ‚âà \\[0.778, 0.806]), **90% = 0.873** (‚âà \\[0.861, 0.884]).\n",
    "* Mean widths: **80% = 0.319**, **90% = 0.428**.\n",
    "* Pinball loss increases smoothly from tails toward the median (table screenshot), consistent with heavier central errors.\n",
    "\n",
    "**Why this matters.**\n",
    "These numbers match my earlier summary: QRF under-covers slightly at 80% and is closer at 90%, with sharp intervals relative to coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067b222",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a372cd",
   "metadata": {},
   "source": [
    "# 2. Calibration & reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03848537",
   "metadata": {},
   "source": [
    "What I did.\n",
    "I evaluated quantile calibration by comparing the predicted quantiles to empirical hit-rates: for each œÑ, I computed \n",
    "ùëù\n",
    "^\n",
    "ùúè\n",
    "=\n",
    "ùëÉ\n",
    "(\n",
    "ùë¶\n",
    "‚â§\n",
    "ùëû\n",
    "^\n",
    "ùúè\n",
    ")\n",
    "p\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    "=P(y‚â§\n",
    "q\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    ") and plotted \n",
    "ùëù\n",
    "^\n",
    "ùúè\n",
    "p\n",
    "^\n",
    "\t‚Äã\n",
    "\n",
    "œÑ\n",
    "\t‚Äã\n",
    "\n",
    " against œÑ with binomial (Wilson) CIs. I produced curves globally and by regime (using my vol_regime; when absent I use width-terciles as a proxy for risk regime). I also summarised interval coverage vs nominal for the 80% and 90% bands, with CIs, and visualised interval width distributions.\n",
    "\n",
    "Why.\n",
    "Reliability curves diagnose systematic under/over-estimation of quantiles, while coverage vs nominal validates the overall calibration of my 80% and 90% intervals. Slicing by regime shows whether mis-calibration concentrates in volatile periods, which informs where conformal offsets or weighting schemes matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e17783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_37332\\4229963903.py:68: UserWarning: Glyph 119875 (\\N{MATHEMATICAL ITALIC CAPITAL P}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_37332\\4229963903.py:69: UserWarning: Glyph 119875 (\\N{MATHEMATICAL ITALIC CAPITAL P}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(OUTDIR / \"fig_reliability_global.png\", dpi=FIG_DPI)\n",
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_37332\\4229963903.py:152: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([q90 - q10, q95 - q05], labels=[\"80% width\",\"90% width\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (WindowsPath('results/tbl_reliability_global.csv'), WindowsPath('results/tbl_reliability_by_regime.csv'), WindowsPath('results/tbl_interval_coverage.csv')) and figures to C:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\notebooks\\Model Building\\results\n"
     ]
    }
   ],
   "source": [
    "# === Step 2 ¬∑ Calibration & reliability ========================================\n",
    "import re, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- CONFIG --------------------------------------------------------------------\n",
    "PRED_PATH = Path(\"qrf_v2_tuned_preds.csv\")   # update if needed\n",
    "OUTDIR = Path(\"results\"); OUTDIR.mkdir(exist_ok=True)\n",
    "FIG_DPI = 140\n",
    "\n",
    "# ---- Load & infer taus ---------------------------------------------------------\n",
    "pred_df = pd.read_csv(PRED_PATH, parse_dates=[\"timestamp\"])\n",
    "assert {\"token\",\"timestamp\",\"y_true\"}.issubset(pred_df.columns)\n",
    "\n",
    "def infer_tau_cols(df):\n",
    "    tau2col = {}\n",
    "    for c in df.columns:\n",
    "        # Match columns like q5, q10, q25, q50, q75, q90, q95\n",
    "        m = re.fullmatch(r\"q(\\d{1,2})\", c)\n",
    "        if m:\n",
    "            tau = int(m.group(1)) / 100.0\n",
    "            tau2col[round(tau, 2)] = c\n",
    "    expected = {0.05,0.10,0.25,0.50,0.75,0.90,0.95}\n",
    "    missing = expected - set(tau2col)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing quantiles {sorted(missing)}. Found: {sorted(tau2col)}\")\n",
    "    return dict(sorted(tau2col.items()))\n",
    "TAU2COL = infer_tau_cols(pred_df)\n",
    "TAUS = list(TAU2COL.keys())\n",
    "\n",
    "# ---- Helpers -------------------------------------------------------------------\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    if n == 0: return (np.nan, np.nan)\n",
    "    z = 1.959963984540054 if alpha==0.05 else 1.2815515655446004\n",
    "    ph = k/n\n",
    "    denom = 1 + z*z/n\n",
    "    centre = (ph + z*z/(2*n)) / denom\n",
    "    half = (z/denom) * np.sqrt((ph*(1-ph) + z*z/(4*n))/n)\n",
    "    return (centre - half, centre + half)\n",
    "\n",
    "# ---- 1) Global reliability: P(y ‚â§ q_tau) vs tau --------------------------------\n",
    "rel_rows = []\n",
    "y = pred_df[\"y_true\"].to_numpy()\n",
    "n_global = len(pred_df)\n",
    "\n",
    "for tau in TAUS:\n",
    "    q = pred_df[TAU2COL[tau]].to_numpy()\n",
    "    hits = (y <= q)\n",
    "    ph = hits.mean()\n",
    "    lo, hi = wilson_ci(hits.sum(), len(hits))\n",
    "    rel_rows.append({\"tau\": tau, \"hit_rate\": float(ph), \"lo\": float(lo), \"hi\": float(hi), \"n\": int(len(hits))})\n",
    "\n",
    "rel_global = pd.DataFrame(rel_rows)\n",
    "rel_global_path = OUTDIR / \"tbl_reliability_global.csv\"\n",
    "rel_global.to_csv(rel_global_path, index=False)\n",
    "\n",
    "# Plot global reliability\n",
    "plt.figure(figsize=(5.2,4.2))\n",
    "plt.plot(rel_global[\"tau\"], rel_global[\"hit_rate\"], marker=\"o\")\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], linestyle=\"--\")  # ideal y=x\n",
    "# error bars\n",
    "plt.errorbar(rel_global[\"tau\"], rel_global[\"hit_rate\"],\n",
    "             yerr=[rel_global[\"hit_rate\"]-rel_global[\"lo\"], rel_global[\"hi\"]-rel_global[\"hit_rate\"]],\n",
    "             fmt=\"none\", capsize=3)\n",
    "plt.xlabel(\"Nominal quantile (œÑ)\")\n",
    "plt.ylabel(\"Empirical hit-rate  ùëÉ(y ‚â§ qÃÇœÑ)\")\n",
    "plt.title(\"Reliability curve ‚Äî Global\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_reliability_global.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 2) Reliability by regime --------------------------------------------------\n",
    "df_reg = pred_df.copy()\n",
    "if \"vol_regime\" in df_reg.columns:\n",
    "    df_reg[\"regime\"] = df_reg[\"vol_regime\"].astype(str)\n",
    "else:\n",
    "    # Fallback proxy: width-terciles of 80% band\n",
    "    width80 = df_reg[TAU2COL[0.90]] - df_reg[TAU2COL[0.10]]\n",
    "    terc = pd.qcut(width80, 3, labels=[\"narrow\",\"mid\",\"wide\"])\n",
    "    df_reg[\"regime\"] = terc.astype(str)\n",
    "\n",
    "rel_reg_rows = []\n",
    "for regime, g in df_reg.groupby(\"regime\"):\n",
    "    y_r = g[\"y_true\"].to_numpy()\n",
    "    for tau in TAUS:\n",
    "        q_r = g[TAU2COL[tau]].to_numpy()\n",
    "        hits = (y_r <= q_r)\n",
    "        ph = hits.mean()\n",
    "        lo, hi = wilson_ci(hits.sum(), len(hits))\n",
    "        rel_reg_rows.append({\"regime\": regime, \"tau\": tau, \"hit_rate\": float(ph),\n",
    "                             \"lo\": float(lo), \"hi\": float(hi), \"n\": int(len(hits))})\n",
    "\n",
    "rel_by_regime = pd.DataFrame(rel_reg_rows)\n",
    "rel_by_regime_path = OUTDIR / \"tbl_reliability_by_regime.csv\"\n",
    "rel_by_regime.to_csv(rel_by_regime_path, index=False)\n",
    "\n",
    "# Plot by regime\n",
    "plt.figure(figsize=(6.2,4.4))\n",
    "for regime, g in rel_by_regime.groupby(\"regime\"):\n",
    "    g = g.sort_values(\"tau\")\n",
    "    plt.plot(g[\"tau\"], g[\"hit_rate\"], marker=\"o\", label=str(regime))\n",
    "plt.plot([min(TAUS), max(TAUS)], [min(TAUS), max(TAUS)], linestyle=\"--\")\n",
    "plt.xlabel(\"Nominal quantile (œÑ)\")\n",
    "plt.ylabel(\"Empirical hit-rate\")\n",
    "plt.title(\"Reliability curve ‚Äî By regime\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_reliability_by_regime.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 3) Interval coverage vs nominal + widths ---------------------------------\n",
    "q05 = pred_df[TAU2COL[0.05]].to_numpy()\n",
    "q10 = pred_df[TAU2COL[0.10]].to_numpy()\n",
    "q90 = pred_df[TAU2COL[0.90]].to_numpy()\n",
    "q95 = pred_df[TAU2COL[0.95]].to_numpy()\n",
    "\n",
    "cover80 = ((y >= q10) & (y <= q90))\n",
    "cover90 = ((y >= q05) & (y <= q95))\n",
    "c80, c90 = cover80.mean(), cover90.mean()\n",
    "c80_lo, c80_hi = wilson_ci(cover80.sum(), len(cover80))\n",
    "c90_lo, c90_hi = wilson_ci(cover90.sum(), len(cover90))\n",
    "w80, w90 = (q90 - q10).mean(), (q95 - q05).mean()\n",
    "\n",
    "cov_tbl = pd.DataFrame({\n",
    "    \"interval\": [\"80%\", \"90%\"],\n",
    "    \"coverage\": [float(c80), float(c90)],\n",
    "    \"lo\": [float(c80_lo), float(c90_lo)],\n",
    "    \"hi\": [float(c80_hi), float(c90_hi)],\n",
    "    \"mean_width\": [float(w80), float(w90)],\n",
    "    \"n\": [int(len(cover80)), int(len(cover90))]\n",
    "})\n",
    "cov_tbl_path = OUTDIR / \"tbl_interval_coverage.csv\"\n",
    "cov_tbl.to_csv(cov_tbl_path, index=False)\n",
    "\n",
    "# Coverage figure with error bars\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "x = np.array([0,1])\n",
    "ybar = cov_tbl[\"coverage\"].to_numpy()\n",
    "yerr = np.vstack([ybar - cov_tbl[\"lo\"].to_numpy(), cov_tbl[\"hi\"].to_numpy() - ybar])\n",
    "plt.errorbar(x, ybar, yerr=yerr, fmt=\"o\", capsize=4)\n",
    "plt.hlines([0.80, 0.90], xmin=-0.3, xmax=1.3, linestyles=[\"--\",\"--\"])\n",
    "plt.xticks(x, cov_tbl[\"interval\"])\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.ylabel(\"Empirical coverage\")\n",
    "plt.title(\"Interval coverage vs nominal\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_interval_coverage.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "# ---- 4) Width distributions (boxplots) -----------------------------------------\n",
    "plt.figure(figsize=(5.2,4.0))\n",
    "plt.boxplot([q90 - q10, q95 - q05], labels=[\"80% width\",\"90% width\"], showfliers=False)\n",
    "plt.ylabel(\"Width\")\n",
    "plt.title(\"Interval width distributions\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTDIR / \"fig_width_distributions.png\", dpi=FIG_DPI)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (rel_global_path, rel_by_regime_path, cov_tbl_path),\n",
    "      \"and figures to\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58611bee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 2 ‚Äî Calibration & reliability (notes)\n",
    "\n",
    "**What the plots show.**\n",
    "\n",
    "* **Global reliability:** œÑ=0.05 and œÑ=0.10 hug y=x (good), but **œÑ=0.25 jumps to \\~0.62** and œÑ=0.50 sits \\~0.74. Upper quantiles (0.75‚Äì0.95) track y=x closely.\n",
    "* **By regime:** the **œÑ=0.25 kink persists across narrow/mid/wide** regimes, so it‚Äôs systematic, not regime-specific.\n",
    "* **Coverage vs nominal:** mirrors the above‚Äîslight under-coverage at 80%, closer at 90%.\n",
    "* **Width distributions:** 90% bands are wider (as expected) with a long right tail during volatile periods.\n",
    "\n",
    "**Diagnosis.**\n",
    "That **large upward kink at œÑ=0.25** points to a calibration bug in my residual shift rule for lower quantiles. In my QRF v3 loop I set the offset for **œÑ<0.5** using **`quantile(residuals, 1 ‚àí œÑ)`**. The correct shift is **`quantile(residuals, œÑ)`** for *all* œÑ. Using `1 ‚àí œÑ` pushes lower quantiles **too high**, inflating hit-rates for œÑ=0.25 (and, via isotonicity, also lifting q50).\n",
    "\n",
    "---\n",
    "\n",
    "## One-line fix to the conformal offsets\n",
    "\n",
    "Replace `1 - tau` with `tau` for all **lower-quantile** branches (both the regime-aware block and the generic block). Here‚Äôs a drop-in replacement for your offset section:\n",
    "\n",
    "```python\n",
    "# --- compute regime-aware Œ¥œÑ on calibration residuals (correct œÑ, not 1-œÑ) -----\n",
    "offsets = np.zeros(len(quantiles))\n",
    "median_bias = np.median(residuals[valid_mask, quantiles.index(0.50)])\n",
    "\n",
    "for qi, tau in enumerate(quantiles):\n",
    "    # winsorize within the valid set\n",
    "    res_all = winsorize_residuals(residuals[valid_mask, qi])\n",
    "\n",
    "    # tails: regime-aware split if available\n",
    "    if tau in [0.05, 0.10, 0.90, 0.95] and 'vol_regime' in df_cal.columns:\n",
    "        quiet_mask = (regime_cal == 'quiet') & valid_mask\n",
    "        vol_mask   = (regime_cal == 'volatile') & valid_mask\n",
    "\n",
    "        def qtau(arr, t=tau):\n",
    "            return np.quantile(winsorize_residuals(arr), t) if arr.size > 0 else np.quantile(res_all, t)\n",
    "\n",
    "        quiet_off = qtau(residuals[quiet_mask, qi])\n",
    "        vol_off   = qtau(residuals[vol_mask, qi])\n",
    "        wq, wv = quiet_mask.sum(), vol_mask.sum()\n",
    "        offsets[qi] = (wq * quiet_off + wv * vol_off) / (wq + wv + 1e-8)\n",
    "\n",
    "    else:\n",
    "        # generic: same rule for all œÑ\n",
    "        offsets[qi] = np.quantile(res_all, tau)\n",
    "\n",
    "# apply Œ¥œÑ and median centering\n",
    "adjusted_test = preds_test + offsets\n",
    "adjusted_test[:, quantiles.index(0.50)] += median_bias\n",
    "adjusted_test = isotonic_non_crossing(adjusted_test, quantiles)\n",
    "```\n",
    "\n",
    "**Why this is correct.**\n",
    "We want $\\mathbb{P}(y \\le \\hat{q}_\\tau + \\delta_\\tau) \\approx \\tau$. With residuals $r = y - \\hat{q}_\\tau$, the shift satisfying this is $\\delta_\\tau = Q_\\tau(r)$, not $Q_{1-\\tau}(r)$.\n",
    "\n",
    "---\n",
    "\n",
    "## What to do now\n",
    "\n",
    "1. Patch the offset code above in your QRF v3 loop (no re-fit needed; it‚Äôs a recalibration step).\n",
    "2. Re-run the evaluation + reliability cells (Steps 1‚Äì2).\n",
    "3. Send me the updated `tbl_interval_coverage.csv` and a snapshot of the **global reliability** plot.\n",
    "\n",
    "If œÑ=0.25 and œÑ=0.50 come down toward the y=x line (they should), we‚Äôll proceed to:\n",
    "\n",
    "* **Step 3:** HAC-robust **Diebold‚ÄìMariano** tests (QRF vs LQR/LightGBM) with a clean per-quantile + per-token table, and\n",
    "* **Step 4:** **Feature pruning** (fold-stability + collinearity trim) and quick re-fit check.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**What I did.**\n",
    "After inspecting reliability curves, I identified a calibration error in my conformal shift rule for lower quantiles. I had incorrectly used $Q_{1-\\tau}(r)$ instead of $Q_{\\tau}(r)$ for residuals $r = y-\\hat{q}_\\tau$. I corrected the offsets to $\\delta_\\tau = Q_{\\tau}(r)$ for all œÑ, keeping the regime-aware split on tails and the isotonic non-crossing step.\n",
    "\n",
    "**Why.**\n",
    "This ensures the adjusted quantiles satisfy $\\mathbb{P}(y \\le \\hat{q}_\\tau) \\approx \\tau$ uniformly across œÑ, preventing the inflated hit-rates previously observed around œÑ=0.25‚Äì0.50 and stabilising median calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8781c0e",
   "metadata": {},
   "source": [
    "## What I did.\n",
    "I audited the volatility regime input used for regime-aware calibration. My feature table encodes vol_regime as an integer quintile in {0,1,2,3,4}, whereas my calibration code expected string labels (‚Äúquiet‚Äù/‚Äúvolatile‚Äù). As a result, the quiet/volatile masks were empty and the tail offsets defaulted to global (or ~zero), i.e. regime-awareness was effectively off. I fixed this by mapping {0,1}‚Üíquiet, {3,4}‚Üívolatile, and {2}‚Üímid, with warm-up NAs assigned to mid. I also retained a fallback that derives regimes from a past-volatility proxy (e.g., gk_vol_36h) if vol_regime is not available.\n",
    "\n",
    "## Why.\n",
    "The purpose of regime-aware calibration is to prevent under-coverage in turbulent periods without widening bands in calm periods. Ensuring the regime signal is recognised by the calibration step is essential; otherwise offsets can be biased toward average conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
