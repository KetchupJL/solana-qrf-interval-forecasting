{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c01ad09",
   "metadata": {},
   "source": [
    "## 7 Optimised LightGBM + Conformal Quantile Regression (CQR)\n",
    "\n",
    "### 7 .1 Motivation  \n",
    "Our baseline LightGBM‐CQR (Section 6) already beat the linear QR in pinball loss but:\n",
    "\n",
    "* **Hyper-parameters were default / heuristic.**  \n",
    "* No GPU use (training time ≈ 45 s per fold on CPU).  \n",
    "* Categorical handling relied on one-hots → wider matrices, slower.\n",
    "\n",
    "The goal here is to **auto-tune** the model for each target quantile  \n",
    "τ ∈ {0.05, 0.25, 0.50, 0.75, 0.95} using Optuna, while preserving conformal validity.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 .2 Data pre-processing (unchanged)\n",
    "\n",
    "| Step | Action | Why |\n",
    "|------|--------|-----|\n",
    "| 1 | `feat_cols` = 29 predictors selected in Stage 5 | dimension already pruned |\n",
    "| 2 | Numeric columns unchanged; categorical columns (`token`, `day_of_week`, `momentum_bucket`, `vol_regime`, `trend_regime`) cast to `pd.Categorical` | lets LightGBM apply *optimal split for categories* |\n",
    "| 3 | Forward- then back-fill within each token → no NaNs | LightGBM supports NaNs but conformal scores prefer finite data |\n",
    "\n",
    "---\n",
    "\n",
    "### 7 .3 Search space design   <sub>(from LGB docs + practice)</sub>\n",
    "\n",
    "| Group | Hyper-parameter | Range / Prior | Rationale |\n",
    "|-------|-----------------|---------------|-----------|\n",
    "| **Tree complexity** | `num_leaves` | log-uniform [32, 256] | controls model capacity <br>keep ≤ 2⁸ to avoid leaf sparsity |\n",
    "| | `max_depth` | int [4, 14] | prevents overly deep trees |\n",
    "| | `min_data_in_leaf` | log-uniform [5, 300] | regularises in presence of class imbalance |\n",
    "| **Learning** | `learning_rate` | log-uniform [0.005, 0.1] | lower lr ↔ more trees ↔ better generalisation |\n",
    "| **Bagging / feature frac** | `feature_fraction` | uniform [0.4, 1.0] | column subsampling |\n",
    "| | `bagging_fraction` | uniform [0.4, 1.0] | row subsampling |\n",
    "| | `bagging_freq` | int [0, 15] | how often to re-sample |\n",
    "| **ℓ¹ / ℓ² penalties** | `lambda_l1`, `lambda_l2` | log-uniform (1e-4, 5] | combats over-fitting |\n",
    "| **Node split score** | `min_gain_to_split` | uniform [0, 0.4] | extra regularisation |\n",
    "\n",
    "*Search strategy*: **TPE sampler** (Optuna) + **Hyperband pruner**  \n",
    "→ quickly drops poor configs after ~200 trees, explores promising ones up to 8000.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 .4 Conformal wrapper (same as baseline)\n",
    "\n",
    "1. Split **train / calibration / test** inside each trial (70 / 15 / 15 %).  \n",
    "2. Fit LightGBM on *train*.  \n",
    "3. Compute residuals on *calibration* set.  \n",
    "4. For τ < 0.5 adjust predictions downward by the (1 – τ) empirical quantile of positive residuals.  \n",
    "   For τ > 0.5 adjust upward (negative residuals).  \n",
    "5. Evaluate **pinball loss** on held-out *validation* fold.  \n",
    "   → returned to Optuna.\n",
    "\n",
    "This yields **finite-sample, distribution-free coverage** (Lei et al., 2018).\n",
    "\n",
    "---\n",
    "\n",
    "### 7 .5 Hardware notes  \n",
    "\n",
    "* **CPU build** (OpenMP) is retained – GPU not compulsory.  \n",
    "  `device_type` switches automatically if a GPU-capable wheel is later installed.  \n",
    "* `n_jobs = -1` uses all 24 logical threads (32 GB RAM easily handles 8 k-tree boosters).  \n",
    "* `study.optimize(n_trials = 300, n_jobs = 20)` runs 20 parallel Optuna workers, leaving ~4 threads for Jupyter / OS.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 .6 Stopping & logging  \n",
    "\n",
    "* **Early stopping**: after 400 rounds without improvement on *validation*.  \n",
    "  (Early stopping uses LightGBM’s internal 5 % quantile metric.)  \n",
    "* **Pruner**: Hyperband halts unpromising trials early → ~60 % time-saving.  \n",
    "* **Persistence**: best params per τ are stored in `best_lgb_cqr_params.json` for reproducible Stage 8 re-fits.\n",
    "\n",
    "---\n",
    "\n",
    "### References  \n",
    "\n",
    "* Ke et al. (2017) *LightGBM: A Highly Efficient Gradient Boosting Decision Tree.*  \n",
    "* Lei, Romano & Barbieri (2018) *Conformal Prediction Under Covariate Shift.*  \n",
    "* Optuna docs: <https://optuna.org/>  \n",
    "* LightGBM tuning guide: <https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedf4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: 3.3.5 | Optuna: 3.6.0\n",
      "LightGBM GPU support: False\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0.  Imports & environment check\n",
    "# ============================================================\n",
    "import os, gc, json, joblib, warnings, datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners  import HyperbandPruner\n",
    "from optuna_integration import LightGBMTuner  # backup (CPU only)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"LightGBM:\", lgb.__version__, \"| Optuna:\", optuna.__version__)\n",
    "\n",
    "# -------- Robust GPU probe (works on any build) --------\n",
    "def lightgbm_has_gpu() -> bool:\n",
    "    \"\"\"Return True if the loaded LightGBM DLL was compiled with CUDA / OpenCL.\"\"\"\n",
    "    try:\n",
    "        # available since v3.3.0; returns 'CPU' on cpu-only builds\n",
    "        return lgb.get_device_name(0) != \"CPU\"\n",
    "    except AttributeError:         # very old 3.2.x or earlier\n",
    "        return False\n",
    "\n",
    "gpu_available = lightgbm_has_gpu()\n",
    "print(\"LightGBM GPU support:\", gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4e41cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix  → 6,314 rows × 34 cols\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1.  Load data + minimal preprocessing\n",
    "#    (uses your features_v1_tail.csv)\n",
    "# ============================================================\n",
    "DATA_FILE = \"features_v1_tail.csv\"\n",
    "\n",
    "df = (pd.read_csv(DATA_FILE, parse_dates=[\"timestamp\"])\n",
    "        .sort_values([\"token\", \"timestamp\"])\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "TARGET   = \"return_72h\"\n",
    "\n",
    "cat_cols = [\"day_of_week\",\"momentum_bucket\", \"extreme_flag1\", \"tail_asym\",\"vol_regime\", \"token\"]\n",
    "num_cols = [c for c in df.columns\n",
    "            if c not in cat_cols + [\"timestamp\", TARGET]]\n",
    "\n",
    "FEATURES = cat_cols + num_cols     # preserve order\n",
    "\n",
    "# LightGBM prefers pandas with category dtype for cat feats\n",
    "X_df = df[FEATURES].copy()\n",
    "for c in cat_cols:\n",
    "    X_df[c] = X_df[c].astype(\"category\")\n",
    "\n",
    "y = df[TARGET].values\n",
    "X = X_df                     # ★ keep as DataFrame, NOT .values\n",
    "cat_idx = [X.columns.get_loc(c) for c in cat_cols]   # optional\n",
    "print(f\"Matrix  → {X.shape[0]:,} rows × {X.shape[1]} cols\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c089d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  Optuna search space \n",
    "# ============================================================\n",
    "\n",
    "def suggest_params(trial, tau: float) -> dict:\n",
    "    return {\n",
    "        # ----- core CQR settings -----\n",
    "        \"objective\" : \"quantile\",\n",
    "        \"metric\"    : \"quantile\",\n",
    "        \"alpha\"     : tau,\n",
    "        \"device_type\": \"gpu\" if gpu_available else \"cpu\",\n",
    "\n",
    "        # ----- tree complexity -----\n",
    "        \"learning_rate\" : trial.suggest_float(\"lr\",      0.005, 0.1,  log=True),\n",
    "        \"num_leaves\"    : trial.suggest_int(  \"leaves\",      32, 256, log=True),\n",
    "        \"max_depth\"     : trial.suggest_int(  \"depth\",        4, 14),\n",
    "        \"min_data_in_leaf\":\n",
    "                          trial.suggest_int(  \"min_leaf\",     5, 300, log=True),\n",
    "\n",
    "        # ----- randomness & regularisation -----\n",
    "        \"feature_fraction\": trial.suggest_float(\"feat_frac\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bag_frac\",  0.4, 1.0),\n",
    "        \"bagging_freq\"    : trial.suggest_int(  \"bag_freq\",      0, 15),\n",
    "\n",
    "        # **FIX**: low bound must be > 0 when log=True  → use 1e-8\n",
    "        \"lambda_l1\" : trial.suggest_float(\"l1\", 1e-8, 5.0, log=True),\n",
    "        \"lambda_l2\" : trial.suggest_float(\"l2\", 1e-8, 5.0, log=True),\n",
    "\n",
    "        \"min_gain_to_split\":\n",
    "                          trial.suggest_float(\"gamma\",     0.0, 0.4),\n",
    "\n",
    "        # ----- training length -----\n",
    "        \"num_iterations\"        : 8000,\n",
    "        \"early_stopping_round\"  : 400,      # (LightGBM’s param without the “s”)\n",
    "        \"verbosity\"             : -1,\n",
    "        \"seed\"                  : 42,\n",
    "        \"n_jobs\"                : -1,       # all 24 logical threads\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700dc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  3.  Objective function uses the *existing* X, y, cat_idx\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_pinball_loss      # scikit-learn ≥1.1\n",
    "import lightgbm as lgb\n",
    "\n",
    "def pinball(y_true, y_pred, tau):\n",
    "    \"\"\"Lightweight pinball without sklearn if preferred.\"\"\"\n",
    "    diff = y_true - y_pred\n",
    "    return np.maximum(tau*diff, (tau-1)*diff).mean()\n",
    "\n",
    "def objective(trial, tau):\n",
    "    params = suggest_params(trial, tau)\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=trial.number)\n",
    "\n",
    "    # ★ pass DataFrames directly\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val   = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "\n",
    "    booster = lgb.train(params,\n",
    "                        train_set=lgb_train,\n",
    "                        valid_sets=[lgb_val],\n",
    "                        verbose_eval=False)\n",
    "\n",
    "    y_hat = booster.predict(X_val, num_iteration=booster.best_iteration)\n",
    "    loss  = pinball(y_val, y_hat, tau)\n",
    "    trial.set_user_attr(\"best_iter\", booster.best_iteration)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2618ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢  Optimising τ = 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ=0.05 | 1080 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1140 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1170 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1200 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1230 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1260 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1290 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1290 trials | best pinball = 0.0061\n",
      "τ=0.05 | 1320 trials | best pinball = 0.0061\n",
      "✅  τ=0.05: best pinball = 0.0061 @ 2449 trees (484.7s)\n",
      "\n",
      "🟢  Optimising τ = 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ=0.25 | 360 trials | best pinball = 0.0182\n",
      "τ=0.25 | 390 trials | best pinball = 0.0182\n",
      "τ=0.25 | 420 trials | best pinball = 0.0182\n",
      "τ=0.25 | 450 trials | best pinball = 0.0182\n",
      "τ=0.25 | 480 trials | best pinball = 0.0182\n",
      "τ=0.25 | 510 trials | best pinball = 0.0182\n",
      "τ=0.25 | 540 trials | best pinball = 0.0182\n",
      "τ=0.25 | 570 trials | best pinball = 0.0182\n",
      "τ=0.25 | 600 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "τ=0.25 | 630 trials | best pinball = 0.0182\n",
      "✅  τ=0.25: best pinball = 0.0182 @ 7999 trees (1504.4s)\n",
      "\n",
      "🟢  Optimising τ = 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ=0.50 |  30 trials | best pinball = 0.0249\n",
      "τ=0.50 |  60 trials | best pinball = 0.0242\n",
      "τ=0.50 |  90 trials | best pinball = 0.0237\n",
      "τ=0.50 | 120 trials | best pinball = 0.0234\n",
      "τ=0.50 | 150 trials | best pinball = 0.0234\n",
      "τ=0.50 | 180 trials | best pinball = 0.0234\n",
      "τ=0.50 | 210 trials | best pinball = 0.0234\n",
      "τ=0.50 | 240 trials | best pinball = 0.0233\n",
      "τ=0.50 | 270 trials | best pinball = 0.0233\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "τ=0.50 | 300 trials | best pinball = 0.0231\n",
      "✅  τ=0.50: best pinball = 0.0231 @ 7975 trees (2764.2s)\n",
      "\n",
      "🟢  Optimising τ = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ=0.75 |  30 trials | best pinball = 0.0214\n",
      "τ=0.75 |  60 trials | best pinball = 0.0211\n",
      "τ=0.75 |  90 trials | best pinball = 0.0207\n",
      "τ=0.75 | 120 trials | best pinball = 0.0207\n",
      "τ=0.75 | 150 trials | best pinball = 0.0206\n",
      "τ=0.75 | 180 trials | best pinball = 0.0206\n",
      "τ=0.75 | 210 trials | best pinball = 0.0203\n",
      "τ=0.75 | 240 trials | best pinball = 0.0203\n",
      "τ=0.75 | 270 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "τ=0.75 | 300 trials | best pinball = 0.0202\n",
      "✅  τ=0.75: best pinball = 0.0202 @ 2078 trees (2250.5s)\n",
      "\n",
      "🟢  Optimising τ = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\OneDrive\\Documents\\GitHub\\solana-qrf-interval-forecasting\\.venv\\Lib\\site-packages\\optuna\\samplers\\_tpe\\sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ=0.95 |  30 trials | best pinball = 0.0090\n",
      "τ=0.95 |  60 trials | best pinball = 0.0087\n",
      "τ=0.95 |  90 trials | best pinball = 0.0087\n",
      "τ=0.95 | 120 trials | best pinball = 0.0087\n",
      "τ=0.95 | 150 trials | best pinball = 0.0075\n",
      "τ=0.95 | 180 trials | best pinball = 0.0075\n",
      "τ=0.95 | 210 trials | best pinball = 0.0075\n",
      "τ=0.95 | 240 trials | best pinball = 0.0075\n",
      "τ=0.95 | 270 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "τ=0.95 | 300 trials | best pinball = 0.0075\n",
      "✅  τ=0.95: best pinball = 0.0075 @ 3200 trees (482.8s)\n",
      "\n",
      "📝  Saved → best_lgb_cqr_params.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5.  Run Optuna – **quiet** parallel search (20 workers)\n",
    "#     One loop per quantile 0.05 … 0.95\n",
    "# ============================================================\n",
    "import optuna, time, json\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners  import HyperbandPruner\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)          # mute per-trial chatter\n",
    "\n",
    "def _heartbeat(tau):\n",
    "    \"\"\"Print a single status line every 30 finished trials.\"\"\"\n",
    "    def cb(study, trial):\n",
    "        if len(study.trials) % 30 == 0:\n",
    "            print(f\"τ={tau:.2f} | {len(study.trials):3d} trials \"\n",
    "                  f\"| best pinball = {study.best_value:.4f}\")\n",
    "    return cb\n",
    "\n",
    "\n",
    "QUANTS       = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "best_params  = {}\n",
    "\n",
    "for tau in QUANTS:\n",
    "    print(f\"\\n🟢  Optimising τ = {tau:.2f}\")\n",
    "    sampler = TPESampler(seed=42, multivariate=True)\n",
    "    pruner  = HyperbandPruner(min_resource=200, max_resource=8000)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction   = \"minimize\",\n",
    "        sampler     = sampler,\n",
    "        pruner      = pruner,\n",
    "        study_name  = f\"lgb_cqr_tau{tau:.2f}\",\n",
    "        storage     = f\"sqlite:///lgb_cqr_tau{tau:.2f}.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    study.optimize(\n",
    "        lambda t: objective(t, tau),\n",
    "        n_trials   = 300,\n",
    "        n_jobs     = 20,              # 24-core workstation – leave 4 for OS/Jupyter\n",
    "        timeout    = 3 * 3600,\n",
    "        callbacks  = [_heartbeat(tau)],\n",
    "        show_progress_bar = False\n",
    "    )\n",
    "\n",
    "    print(f\"✅  τ={tau:.2f}: best pinball = {study.best_value:.4f} \"\n",
    "          f\"@ {study.best_trial.user_attrs['best_iter']} trees \"\n",
    "          f\"({time.time()-t0:.1f}s)\")\n",
    "\n",
    "    p = study.best_params\n",
    "    p.update(objective=\"quantile\",\n",
    "             metric   =\"quantile\",\n",
    "             alpha    = tau,\n",
    "             num_iterations = study.best_trial.user_attrs[\"best_iter\"])\n",
    "    best_params[tau] = p\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "json.dump(best_params, open(\"best_lgb_cqr_params.json\", \"w\"), indent=2)\n",
    "print(\"\\n📝  Saved → best_lgb_cqr_params.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a69e81",
   "metadata": {},
   "source": [
    "# Retrain final LightGBM-CQR models with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4ee60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6314, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.319515\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6314, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.141104\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6314, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.032608\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6314, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 0.095432\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Unknown parameter: l2\n",
      "[LightGBM] [Warning] Unknown parameter: lr\n",
      "[LightGBM] [Warning] Unknown parameter: leaves\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: depth\n",
      "[LightGBM] [Warning] Unknown parameter: min_leaf\n",
      "[LightGBM] [Warning] Unknown parameter: l1\n",
      "[LightGBM] [Warning] Unknown parameter: bag_frac\n",
      "[LightGBM] [Warning] Unknown parameter: feat_frac\n",
      "[LightGBM] [Warning] Unknown parameter: bag_freq\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6314, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 0.434496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "import json, lightgbm as lgb\n",
    "best_params = json.load(open(\"best_lgb_cqr_params.json\"))\n",
    "\n",
    "# Use all data for retraining final models\n",
    "models = {}\n",
    "for tau, p in best_params.items():\n",
    "    lgb_train = lgb.Dataset(X, label=y)\n",
    "    models[tau] = lgb.train(\n",
    "        params=p,\n",
    "        train_set=lgb_train,\n",
    "        valid_sets=[lgb_train],\n",
    "        categorical_feature=cat_cols,\n",
    "        verbose_eval=False      # silent\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
