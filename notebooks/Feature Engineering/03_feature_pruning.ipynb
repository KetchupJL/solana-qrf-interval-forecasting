{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f7165d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5320bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "df = pd.read_parquet(\"C:/Users/james/OneDrive/Documents/GitHub/solana-qrf-interval-forecasting/data/features_full.parquet\").dropna(subset=[\"return_72h\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f619b19",
   "metadata": {},
   "source": [
    "## Stage 1 – define predictors & basic types\n",
    "anything that's not timestamp, token, or the 72-h target is a candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bfb9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 numeric and 8 categorical predictors.\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [\"timestamp\", \"token\", \"return_72h\"]\n",
    "predictors_initial = df.columns.difference(drop_cols).tolist()\n",
    "\n",
    "# Columns that are really *categorical* even if stored as int/float\n",
    "EXPLICIT_CAT = [\n",
    "    # seasonality\n",
    "    \"day_of_week\",\n",
    "    # discretised regimes\n",
    "    \"vol_regime\",          # 0–4\n",
    "    \"trend_regime\",        # -1, 0, +1\n",
    "    \"momentum_bucket\",     # 0–9\n",
    "    # extreme-tail indicators (binary)\n",
    "    \"extreme_flag1\", \"tail_positive\", \"tail_negative\", \"tail_asym\"\n",
    "]\n",
    "\n",
    "for col in EXPLICIT_CAT:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Everything else stays numeric\n",
    "#   hour_sin / hour_cos are **already scaled floats** – leave numeric.\n",
    "#   holder_missing, new_token_accounts_missing etc. are binary ints – fine as numeric.\n",
    "\n",
    "# 2.  Build initial predictor lists\n",
    "drop_cols = [\"timestamp\", \"token\", \"return_72h\"]\n",
    "cat_feats = [c for c in df.columns\n",
    "             if df[c].dtype.name == \"category\" and c not in drop_cols]\n",
    "num_feats = [c for c in df.columns\n",
    "             if c not in drop_cols + cat_feats]\n",
    "\n",
    "print(f\"Found {len(num_feats)} numeric and {len(cat_feats)} categorical predictors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01eded4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 keeps 41 predictors (40 numeric ➞ 33 kept, 8 categorical ➞ 8 kept)\n"
     ]
    }
   ],
   "source": [
    "# 3.  Variance & sparsity filter  (numeric variance; both sparsity)\n",
    "#     • keep numeric if var > 1e-8\n",
    "#     • keep any feature (num or cat) if <80 % NaN\n",
    "\n",
    "var_keep = df[num_feats].var() > 1e-8          # Series indexed by num_feats\n",
    "sparse_keep = df[num_feats + cat_feats].isna().mean() < 0.80\n",
    "\n",
    "predictors_stage1 = (\n",
    "      var_keep.index[var_keep & sparse_keep[var_keep.index]].tolist()   # numeric pass\n",
    "    + [c for c in cat_feats if sparse_keep[c]]                          # cat pass\n",
    ")\n",
    "\n",
    "print(f\"Stage 1 keeps {len(predictors_stage1)} predictors \"\n",
    "      f\"({len(num_feats)} numeric ➞ {var_keep.sum()} kept, \"\n",
    "      f\"{len(cat_feats)} categorical ➞ {len([c for c in cat_feats if sparse_keep[c]])} kept)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8f236",
   "metadata": {},
   "source": [
    "Stage 1 — Type Cleaning & Basic Filters\n",
    "Objective Ensure that every candidate predictor is (i) assigned the correct data type for downstream models and (ii) carries non-trivial information.\n",
    "\n",
    "Explicit categorical encoding\n",
    "day_of_week, vol_regime, trend_regime, momentum_bucket, plus binary extreme flags were force-cast to pandas category.\n",
    "This allows LightGBM to apply native categorical split logic and keeps open the option of one-hot expansion for linear models.\n",
    "\n",
    "Near-zero variance filter \n",
    "Numeric predictors with variance ≤ 1 × 10⁻⁸ were removed, as they provide no discriminatory power and can destabilise optimisation in linear quantile regression.\n",
    "\n",
    "Sparsity filter \n",
    "Any feature (numeric or categorical) with ≥ 80 % missing values was discarded.\n",
    "This threshold balances the gain from rare-event signals (e.g. extreme_flag) against the risk of noise amplification in high-NA columns.\n",
    "\n",
    "Result ≈ N₁ predictors ( n₁ numeric | c₁ categorical ) passed Stage 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eedfadb",
   "metadata": {},
   "source": [
    "## Stage 2 – multicollinearity filter (|ρ| > 0.98 → drop one feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ab8b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1 highly-collinear numerics (>0.98) ➜ 40 predictors remain.\n",
      "Numeric kept: 32  |  Categorical kept: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['williams_r']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# ① split Stage-1 list back into numeric vs. categorical\n",
    "num_keep = [c for c in predictors_stage1 if c in num_feats]\n",
    "cat_keep = [c for c in predictors_stage1 if c in cat_feats]\n",
    "\n",
    "# ② compute absolute Pearson correlation on numeric part\n",
    "corr = df[num_keep].corr().abs()\n",
    "\n",
    "# ③ scan the upper triangle; mark the *second* feature for dropping\n",
    "to_drop = set()\n",
    "for (col_i, col_j) in combinations(corr.columns, 2):\n",
    "    if corr.loc[col_i, col_j] > 0.98:\n",
    "        # keep the first occurrence, drop the second\n",
    "        to_drop.add(col_j)\n",
    "\n",
    "num_after = [c for c in num_keep if c not in to_drop]\n",
    "predictors_stage2 = num_after + cat_keep\n",
    "\n",
    "print(f\"Dropped {len(to_drop)} highly-collinear numerics \"\n",
    "      f\"(>0.98) ➜ {len(predictors_stage2)} predictors remain.\\n\"\n",
    "      f\"Numeric kept: {len(num_after)}  |  Categorical kept: {len(cat_keep)}\")\n",
    "\n",
    "# Optional: inspect what was dropped\n",
    "display(sorted(to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de585939",
   "metadata": {},
   "source": [
    "Stage 2 — Multicollinearity Filter\n",
    "Objective: remove redundant numeric predictors to stabilise models and clarify feature importance.\n",
    "\n",
    "Procedure\n",
    "\n",
    "Compute the absolute Pearson correlation matrix for the numeric Stage-1 set.\n",
    "\n",
    "Scan the upper triangle; when |ρ| > 0 .98, keep the first feature \n",
    " \n",
    "The 0.98 cut-off prunes near-duplicates without sacrificing distinct information.\n",
    "\n",
    "Categorical variables are exempt—Pearson’s ρ does not capture their dependence.\n",
    "\n",
    "Outcome N₂ = n₂ + c₁ predictors survive (n₂ numeric | c₁ categorical) and feed Stage 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772f40e",
   "metadata": {},
   "source": [
    "## Stage 3 — Cheap LightGBM Quantile Model (τ = 0.50)\n",
    "\n",
    "**Objective** Obtain a fast, model-based ranking of predictor importance  \n",
    "before engaging in computationally expensive tuning.\n",
    "\n",
    "* **Model**  LightGBM with `objective=\"quantile\"` and `alpha = 0.5`\n",
    "  (i.e., median pinball loss).  \n",
    "* **Configuration**  400 trees, shrinkage 0.05, moderate regularisation\n",
    "  (`num_leaves = 64`, 80 % row/feature bagging).  \n",
    "* **Categorical handling**  Native LightGBM categorical splits, using the\n",
    "  list derived in Stage 1 (`cat_keep`).  \n",
    "* **Output**  Gain-based importance for every predictor; features\n",
    "  contributing < 0.3 % total gain will be eligible for pruning in\n",
    "  Stage 4.\n",
    "\n",
    "Running this quick model costs < 30 s on a laptop yet yields a robust\n",
    "importance ordering that reliably mirrors more expensive\n",
    "hyper-parameter-tuned runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e70112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. prepare matrices -------------------------------------------------\n",
    "X = df[predictors_stage2]          # predictors from Stage 2\n",
    "y = df[\"return_72h\"]\n",
    "\n",
    "lgb_data = lgb.Dataset(\n",
    "    X,\n",
    "    label=y,\n",
    "    categorical_feature=cat_keep,  # defined in Stage 1\n",
    "    free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e053112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. model params -----------------------------------------------------\n",
    "params = dict(\n",
    "    objective        = \"quantile\",\n",
    "    alpha            = 0.5,          # median\n",
    "    learning_rate    = 0.05,\n",
    "    num_leaves       = 64,\n",
    "    feature_fraction = 0.80,\n",
    "    bagging_fraction = 0.80,\n",
    "    seed             = 42,\n",
    "    verbose          = -1,\n",
    ")\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_data,\n",
    "    num_boost_round = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c55d6f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_85b66\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_85b66_level0_col0\" class=\"col_heading level0 col0\" >gain_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row0\" class=\"row_heading level0 row0\" >proc</th>\n",
       "      <td id=\"T_85b66_row0_col0\" class=\"data row0 col0\" >32.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row1\" class=\"row_heading level0 row1\" >ret_ETH</th>\n",
       "      <td id=\"T_85b66_row1_col0\" class=\"data row1 col0\" >4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row2\" class=\"row_heading level0 row2\" >ret_SOL</th>\n",
       "      <td id=\"T_85b66_row2_col0\" class=\"data row2 col0\" >4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row3\" class=\"row_heading level0 row3\" >ret_BTC</th>\n",
       "      <td id=\"T_85b66_row3_col0\" class=\"data row3 col0\" >3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row4\" class=\"row_heading level0 row4\" >stoch_k</th>\n",
       "      <td id=\"T_85b66_row4_col0\" class=\"data row4 col0\" >3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row5\" class=\"row_heading level0 row5\" >cci</th>\n",
       "      <td id=\"T_85b66_row5_col0\" class=\"data row5 col0\" >3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row6\" class=\"row_heading level0 row6\" >logret_12h</th>\n",
       "      <td id=\"T_85b66_row6_col0\" class=\"data row6 col0\" >3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row7\" class=\"row_heading level0 row7\" >logret_36h</th>\n",
       "      <td id=\"T_85b66_row7_col0\" class=\"data row7 col0\" >3.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row8\" class=\"row_heading level0 row8\" >bollinger_bw</th>\n",
       "      <td id=\"T_85b66_row8_col0\" class=\"data row8 col0\" >3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row9\" class=\"row_heading level0 row9\" >bollinger_b</th>\n",
       "      <td id=\"T_85b66_row9_col0\" class=\"data row9 col0\" >3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row10\" class=\"row_heading level0 row10\" >adx</th>\n",
       "      <td id=\"T_85b66_row10_col0\" class=\"data row10 col0\" >2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row11\" class=\"row_heading level0 row11\" >vol_std_7bar</th>\n",
       "      <td id=\"T_85b66_row11_col0\" class=\"data row11 col0\" >2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row12\" class=\"row_heading level0 row12\" >vol_zscore_14</th>\n",
       "      <td id=\"T_85b66_row12_col0\" class=\"data row12 col0\" >2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row13\" class=\"row_heading level0 row13\" >skew_36h</th>\n",
       "      <td id=\"T_85b66_row13_col0\" class=\"data row13 col0\" >2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row14\" class=\"row_heading level0 row14\" >tx_per_account</th>\n",
       "      <td id=\"T_85b66_row14_col0\" class=\"data row14 col0\" >2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row15\" class=\"row_heading level0 row15\" >holder_growth_1bar</th>\n",
       "      <td id=\"T_85b66_row15_col0\" class=\"data row15 col0\" >2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row16\" class=\"row_heading level0 row16\" >parkinson_vol_36h</th>\n",
       "      <td id=\"T_85b66_row16_col0\" class=\"data row16 col0\" >2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row17\" class=\"row_heading level0 row17\" >gk_vol_36h</th>\n",
       "      <td id=\"T_85b66_row17_col0\" class=\"data row17 col0\" >2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row18\" class=\"row_heading level0 row18\" >holder_growth_7d</th>\n",
       "      <td id=\"T_85b66_row18_col0\" class=\"data row18 col0\" >2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85b66_level0_row19\" class=\"row_heading level0 row19\" >downside_vol_3bar</th>\n",
       "      <td id=\"T_85b66_row19_col0\" class=\"data row19 col0\" >2.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1adad442ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 3 complete → 29 predictors (cover 99.3% of total gain) advance to Stage 4.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. gain importance --------------------------------------------------\n",
    "gain = pd.Series(\n",
    "    gbm.feature_importance(importance_type=\"gain\"),\n",
    "    index = predictors_stage2\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "gain_pct = 100 * gain / gain.sum()\n",
    "display(gain_pct.head(20).to_frame(\"gain_%\").style.format({\"gain_%\":\"{:.2f}\"}))\n",
    "\n",
    "# candidate list for Stage 4 pruning\n",
    "threshold = 0.3                   # % of total gain\n",
    "predictors_stage3 = gain_pct[gain_pct >= threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nStage 3 complete → {len(predictors_stage3)} predictors \"\n",
    "      f\"(cover {gain_pct[gain_pct >= threshold].sum():.1f}% of total gain) \"\n",
    "      \"advance to Stage 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a1575",
   "metadata": {},
   "source": [
    "### Stage 4 — Gain-Based Feature Pruning\n",
    "\n",
    "**Objective** Remove predictors that contribute a negligible share of LightGBM gain so subsequent\n",
    "hyper-parameter search is faster and feature importance clearer.\n",
    "\n",
    "* **Criterion**  \n",
    "  A predictor is kept if its **gain share ≥ 0.3 %** of total model gain\n",
    "  (median-quantile LightGBM from Stage 3).\n",
    "\n",
    "* **Result**  \n",
    "  *29 predictors* survive the filter, representing **99.3 % of total gain**.\n",
    "  The discarded set contains mainly rare-event flags (`extreme_flag1`,\n",
    "  `tail_*`) and low-signal regime dummies (`vol_regime`, `trend_regime`)\n",
    "  that LightGBM could not exploit at τ = 0.5.\n",
    "\n",
    "* **Rationale**  \n",
    "  • 0.3 % is conservative: features below this level each explain less\n",
    "    than 1⁄300 of model gain.  \n",
    "  • Sparse tail flags can still be revisited for τ = 0.10 / 0.90 if needed,\n",
    "    but including them now would inflate tree depth without measurable\n",
    "    benefit at the median.\n",
    "\n",
    "The pruned predictor list is frozen as **feature-set v1** for all\n",
    "down-stream modelling and statistical tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "084192df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 29 predictors (covers 99.3% of gain)\n",
      "📁  Saved feature-set v1 → features_v1.parquet\n"
     ]
    }
   ],
   "source": [
    "THRESH = 0.3    # percent gain threshold (feel free to tweak)\n",
    "\n",
    "predictors_final = gain_pct[gain_pct >= THRESH].index.tolist()\n",
    "print(f\"Kept {len(predictors_final)} predictors \"\n",
    "      f\"(covers {gain_pct[gain_pct >= THRESH].sum():.1f}% of gain)\")\n",
    "\n",
    "# ----- build final dataframe to save -------------------------------------\n",
    "cols_to_save = [\"timestamp\", \"token\", \"return_72h\"] + predictors_final\n",
    "df_v1        = df[cols_to_save].copy()\n",
    "\n",
    "df_v1.to_parquet(\"features_v1.parquet\", index=False)\n",
    "print(\"📁  Saved feature-set v1 → features_v1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725aefa",
   "metadata": {},
   "source": [
    "## Stage 5 — Domain “must-keep” Add-Backs\n",
    "\n",
    "Sparse tail-event indicators carry little gain for the median\n",
    "quantile, but economic theory suggests they matter for the tails\n",
    "(τ ≪ 0.50 or τ ≫ 0.50).  \n",
    "Therefore, we add back  \n",
    "`extreme_flag`, `tail_pos`, `tail_neg`, `tail_asym`,  \n",
    "`extreme_count_72h`  \n",
    "after Stage 4 pruning.  These flags cost almost no depth in tree models\n",
    "and can widen the 10 % / 90 % (and other tail) intervals when recent\n",
    "shocks cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc7cc2",
   "metadata": {},
   "source": [
    "## Stage 6 — Freeze Feature Sets\n",
    "\n",
    "We freeze two predictor sets for reproducibility:\n",
    "\n",
    "| File | # Predictors | Intended quantiles |\n",
    "|------|--------------|--------------------|\n",
    "| `features_v1.parquet` | 29 | core set for τ = 0.50 (median) baseline models |\n",
    "| `features_v1_tail.parquet` | 34 | core set **plus** tail flags for τ ∈ {0.05, 0.10, 0.25, 0.75, 0.90, 0.95} models |\n",
    "\n",
    "Locking the sets guarantees fair, identical inputs in all subsequent\n",
    "rolling-CV experiments and statistical tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c070d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-set sizes  →  v1: 29  |  v1_tail: 32\n",
      "✅  Saved features_v1.parquet  and  features_v1_tail.parquet\n"
     ]
    }
   ],
   "source": [
    "# 1. Add tail flags to the pruned predictor list\n",
    "tail_cols = [\"extreme_flag1\",\"tail_asym\", \"extreme_count_72h\"]\n",
    "tail_cols  = [c for c in tail_cols if c in df.columns]\n",
    "\n",
    "predictors_final_tail = predictors_final + tail_cols\n",
    "\n",
    "print(f\"Feature-set sizes  →  v1: {len(predictors_final)}  |  v1_tail: {len(predictors_final_tail)}\")\n",
    "\n",
    "# 2. Save Parquet files\n",
    "base_cols = [\"timestamp\", \"token\", \"return_72h\"]\n",
    "\n",
    "df[base_cols + predictors_final]         .to_parquet(\"features_v1.parquet\",       index=False)\n",
    "df[base_cols + predictors_final_tail]    .to_parquet(\"features_v1_tail.parquet\", index=False)\n",
    "\n",
    "print(\"✅  Saved features_v1.parquet  and  features_v1_tail.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
